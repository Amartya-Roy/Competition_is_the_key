{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHy-Kl3rbCID",
        "outputId": "8466cf14-fe66-4e4f-9e2b-f61473c49899"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gcastle\n",
            "  Downloading gcastle-1.0.4-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from gcastle) (3.10.0)\n",
            "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.11/dist-packages (from gcastle) (1.16.1)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.11/dist-packages (from gcastle) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.2->gcastle) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.2->gcastle) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.2->gcastle) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.2->gcastle) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.2->gcastle) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.2->gcastle) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.2->gcastle) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading gcastle-1.0.4-py3-none-any.whl (214 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.4/214.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m108.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, gcastle\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed gcastle-1.0.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install gcastle torch numpy pandas networkx scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "SKAh4NMWz3Rx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Causal-Discovery (DQN vs. GraN-DAG) with Copula/ Gaussian BIC, CAM pruning, live metrics\n",
        "\n",
        "- Fast Gaussian BIC and Gaussian Copula (nonparanormal) BIC\n",
        "- Greedy warm-start (beam + reversals)\n",
        "- Double DQN + Polyak target updates\n",
        "- CAM pruning (linear regression) post-training\n",
        "- SHD/FDR/TPR/etc printed every eval, plus best-by-ValBIC and best-by-TPR snapshots\n",
        "\n",
        "Training does NOT use GT. GT (if provided) is used ONLY for evaluation.\n",
        "\"\"\"\n",
        "\n",
        "import os, random, warnings\n",
        "from collections import deque\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import networkx as nx\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from scipy.stats import rankdata, norm\n",
        "\n",
        "# Opponent\n",
        "from castle.algorithms import GraNDAG\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# -------------------- Config --------------------\n",
        "DATA_CSV   = \"/content/data.csv\"\n",
        "GT_NPY     = \"/content/adj.npy\"    # optional; evaluation only\n",
        "G_ITER     = 1000                  # GraN-DAG iterations\n",
        "\n",
        "N_EPISODES = 400\n",
        "EVAL_EVERY = 20\n",
        "SEED       = 42\n",
        "\n",
        "EDGE_BUDGET_RATIO = 1.1           # warm-start cap (~1.1 * p edges)\n",
        "LAMBDA_L1         = 0.02          # sparsity penalty in reward\n",
        "ACTION_COST       = 0.05          # small penalty per committed edit\n",
        "CAM_TH            = 0.3           # CAM pruning threshold\n",
        "\n",
        "SCORE_TYPE = \"copula\"             # \"copula\" (robust) or \"gaussian\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DTYPE  = torch.double\n",
        "np.random.seed(SEED); random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "\n",
        "# -------------------- IO --------------------\n",
        "def load_data(csv_path=DATA_CSV):\n",
        "    if not os.path.exists(csv_path):\n",
        "        print(f\"Error: '{csv_path}' not found.\"); return None\n",
        "    df = pd.read_csv(csv_path, header=0)\n",
        "    if df.columns[0].lower().startswith(\"unnamed\"):\n",
        "        df = pd.read_csv(csv_path, header=0, index_col=0)\n",
        "    df = df.apply(pd.to_numeric, errors=\"coerce\").dropna(axis=1, how=\"all\")\n",
        "    X = df.values.astype(np.float64)\n",
        "    mu, sd = X.mean(0, keepdims=True), X.std(0, keepdims=True); sd[sd == 0] = 1.0\n",
        "    X = (X - mu) / sd\n",
        "    print(f\"Loaded data: {X.shape[0]} samples, {X.shape[1]} vars\")\n",
        "    return X\n",
        "\n",
        "def load_truth(npy_path=GT_NPY, p=None):\n",
        "    if not os.path.exists(npy_path):\n",
        "        print(\"No ground truth file; metrics will be limited.\")\n",
        "        return None\n",
        "    G = np.load(npy_path).astype(np.float64)\n",
        "    if p is not None and G.shape != (p, p):\n",
        "        print(f\"[align] trimming GT from {G.shape} to {(p,p)}\")\n",
        "        G = G[:p, :p]\n",
        "    print(\"Loaded ground truth:\", G.shape)\n",
        "    return G\n",
        "\n",
        "\n",
        "# -------------------- Opponent (GraN-DAG) --------------------\n",
        "def get_grandag_adj(data, iterations=G_ITER, hidden_dim=16, hidden_num=2, lr=5e-4, h_threshold=1e-6):\n",
        "    print(f\"\\nRunning GraN-DAG (iterations={iterations})...\")\n",
        "    model = GraNDAG(\n",
        "        input_dim=data.shape[1], hidden_dim=hidden_dim, hidden_num=hidden_num,\n",
        "        lr=lr, iterations=iterations, h_threshold=h_threshold, mu_init=1e-3\n",
        "    )\n",
        "    model.learn(data)\n",
        "    print(\"GraN-DAG done.\")\n",
        "    return np.array(model.causal_matrix, dtype=np.float64)\n",
        "\n",
        "\n",
        "# -------------------- Metrics --------------------\n",
        "def binarize(A):\n",
        "    A = (A != 0).astype(int)\n",
        "    np.fill_diagonal(A, 0)\n",
        "    return A\n",
        "\n",
        "def shd_binary(A, B):\n",
        "    A = binarize(A); B = binarize(B)\n",
        "    Au = ((A + A.T) > 0).astype(int); Bu = ((B + B.T) > 0).astype(int)\n",
        "    undirected_diff = int(np.sum(np.triu(Au ^ Bu, 1)))\n",
        "    common_u = ((Au & Bu) > 0).astype(int)\n",
        "    orient_mismatch = int(np.sum(np.triu((A ^ B) & common_u, 1)))\n",
        "    return undirected_diff + orient_mismatch\n",
        "\n",
        "def eval_against_gt(pred_adj, GT):\n",
        "    if GT is None or pred_adj.shape != GT.shape:\n",
        "        return None\n",
        "    P = binarize(pred_adj); T = binarize(GT)\n",
        "    tp = int(((P == 1) & (T == 1)).sum())\n",
        "    fp = int(((P == 1) & (T == 0)).sum())\n",
        "    fn = int(((P == 0) & (T == 1)).sum())\n",
        "    tn = int(((P == 0) & (T == 0)).sum())\n",
        "    fdr = fp / max(tp + fp, 1)\n",
        "    tpr = tp / max(tp + fn, 1)\n",
        "    fpr = fp / max(fp + tn, 1)\n",
        "    return {\n",
        "        \"total_edges_original\":  int(T.sum()),\n",
        "        \"total_edges_predicted\": int(P.sum()),\n",
        "        \"correct_edges\":         tp,\n",
        "        \"fdr\": round(fdr, 4),\n",
        "        \"tpr\": round(tpr, 4),\n",
        "        \"fpr\": round(fpr, 4),\n",
        "        \"shd\": shd_binary(P, T),\n",
        "        \"nnz\": int(P.sum()),\n",
        "    }\n",
        "\n",
        "\n",
        "# -------------------- Scorers --------------------\n",
        "class GaussianBIC:\n",
        "    \"\"\"Gaussian BIC via covariance blocks (no per-step OLS).\"\"\"\n",
        "    def __init__(self, X):\n",
        "        X = np.asarray(X, dtype=np.float64)\n",
        "        self.X = X\n",
        "        self.n, self.p = X.shape\n",
        "        self.S = X.T @ X\n",
        "        self.yTy = np.diag(self.S)\n",
        "\n",
        "    def node_rss(self, parents, j):\n",
        "        if len(parents) == 0:\n",
        "            y = self.X[:, j]\n",
        "            return float(((y - y.mean()) ** 2).sum())\n",
        "        P = np.array(parents, dtype=int)\n",
        "        Spp = self.S[np.ix_(P, P)]\n",
        "        Spy = self.S[P, j]\n",
        "        try:\n",
        "            coef = np.linalg.solve(Spp, Spy)\n",
        "        except np.linalg.LinAlgError:\n",
        "            coef = np.linalg.pinv(Spp) @ Spy\n",
        "        rss = self.yTy[j] - Spy @ coef\n",
        "        return float(max(rss, 1e-12))\n",
        "\n",
        "    def bic(self, A):\n",
        "        n, p = self.n, self.p\n",
        "        total_rss = 0.0\n",
        "        num_params = p  # intercepts\n",
        "        for j in range(p):\n",
        "            parents = np.where(A[:, j] == 1)[0].tolist()\n",
        "            total_rss += self.node_rss(parents, j)\n",
        "            num_params += len(parents)\n",
        "        dof = max(n - num_params, 1)\n",
        "        sigma2 = max(total_rss / dof, 1e-9)\n",
        "        loglik = -0.5 * n * (p * np.log(2 * np.pi * sigma2) + 1.0)\n",
        "        return float(loglik - 0.5 * num_params * np.log(n))\n",
        "\n",
        "def gaussian_copula_transform(X):\n",
        "    \"\"\"\n",
        "    Nonparanormal transform: map each marginal to ~N(0,1) by rank -> Phi^-1(u).\n",
        "    \"\"\"\n",
        "    X = np.asarray(X, dtype=np.float64)\n",
        "    n, p = X.shape\n",
        "    Z = np.empty_like(X)\n",
        "    eps = 1e-6\n",
        "    rng = np.random.default_rng(0)\n",
        "    for j in range(p):\n",
        "        x = X[:, j]\n",
        "        if np.std(x) < 1e-12:\n",
        "            x = x + rng.normal(0, 1e-9, size=n)\n",
        "        r = rankdata(x, method=\"average\")  # 1..n\n",
        "        u = (r - 0.5) / n                  # (0,1)\n",
        "        u = np.clip(u, eps, 1 - eps)\n",
        "        Z[:, j] = norm.ppf(u)\n",
        "    Z -= Z.mean(axis=0, keepdims=True)\n",
        "    std = Z.std(axis=0, keepdims=True); std[std == 0] = 1.0\n",
        "    Z /= std\n",
        "    return Z\n",
        "\n",
        "class CopulaBIC(GaussianBIC):\n",
        "    \"\"\"Gaussian Copula (Nonparanormal) BIC scorer.\"\"\"\n",
        "    def __init__(self, X):\n",
        "        Z = gaussian_copula_transform(X)\n",
        "        super().__init__(Z)\n",
        "\n",
        "\n",
        "# -------------------- Warm-start (greedy + reversals) --------------------\n",
        "def warm_start_greedy_bic(Xtr, Xva, edge_budget, scorer_cls=GaussianBIC,\n",
        "                          max_passes=5, topk_per_pass=5, restarts=5, seed=SEED):\n",
        "    rng = np.random.RandomState(seed)\n",
        "    p = Xtr.shape[1]\n",
        "    fb = scorer_cls(Xva)\n",
        "\n",
        "    def is_dag(M): return nx.is_directed_acyclic_graph(nx.DiGraph(M))\n",
        "\n",
        "    def one_run():\n",
        "        A = np.zeros((p, p), dtype=np.float64)\n",
        "        best = fb.bic(A)\n",
        "        for _ in range(max_passes):\n",
        "            improved = False\n",
        "            # forward: add Top-K\n",
        "            cands = []\n",
        "            if A.sum() < edge_budget:\n",
        "                for i in range(p):\n",
        "                    for j in range(p):\n",
        "                        if i == j or A[i, j] == 1: continue\n",
        "                        if A.sum() >= edge_budget: break\n",
        "                        trial = A.copy(); trial[i, j] = 1.0; np.fill_diagonal(trial, 0.0)\n",
        "                        if not is_dag(trial): continue\n",
        "                        s = fb.bic(trial)\n",
        "                        if s > best: cands.append((s - best, i, j))\n",
        "                cands.sort(reverse=True, key=lambda x: x[0])\n",
        "                for _, i, j in cands[:topk_per_pass]:\n",
        "                    if A.sum() >= edge_budget: break\n",
        "                    trial = A.copy(); trial[i, j] = 1.0; np.fill_diagonal(trial, 0.0)\n",
        "                    if not is_dag(trial): continue\n",
        "                    s = fb.bic(trial)\n",
        "                    if s > best:\n",
        "                        A, best, improved = trial, s, True\n",
        "            # backward: prune\n",
        "            pruned = True\n",
        "            while pruned:\n",
        "                pruned = False\n",
        "                for i in range(p):\n",
        "                    for j in range(p):\n",
        "                        if A[i, j] == 0: continue\n",
        "                        trial = A.copy(); trial[i, j] = 0.0\n",
        "                        s = fb.bic(trial)\n",
        "                        if s > best:\n",
        "                            A, best, improved, pruned = trial, s, True, True\n",
        "            # reversal sweep\n",
        "            for i in range(p):\n",
        "                for j in range(p):\n",
        "                    if A[i, j] != 1: continue\n",
        "                    trial = A.copy(); trial[i, j] = 0.0; trial[j, i] = 1.0\n",
        "                    if not is_dag(trial): continue\n",
        "                    s = fb.bic(trial)\n",
        "                    if s > best:\n",
        "                        A, best, improved = trial, s, True\n",
        "            if not improved: break\n",
        "        return A, best\n",
        "\n",
        "    bestA, bestS = None, -np.inf\n",
        "    for _ in range(restarts):\n",
        "        A, s = one_run()\n",
        "        if s > bestS: bestA, bestS = A, s\n",
        "    return bestA\n",
        "\n",
        "\n",
        "# -------------------- Environment --------------------\n",
        "class CausalDiscoveryEnv:\n",
        "    \"\"\"\n",
        "    Reward = Δ Val-BIC - λ1 * edges - action_cost - small step penalty.\n",
        "    \"\"\"\n",
        "    def __init__(self, data, grandag_adj,\n",
        "                 val_frac=0.2,\n",
        "                 edge_budget_ratio=EDGE_BUDGET_RATIO,\n",
        "                 lambda_l1=LAMBDA_L1, action_cost=ACTION_COST,\n",
        "                 warm_start=True,\n",
        "                 score_type=SCORE_TYPE):\n",
        "        self.full = data\n",
        "        self.n_samples, self.n_nodes = data.shape\n",
        "        self.grandag_adj = grandag_adj.astype(np.float64)\n",
        "\n",
        "        idx = np.arange(self.n_samples); rng = np.random.RandomState(SEED)\n",
        "        rng.shuffle(idx)\n",
        "        cut = int((1.0 - val_frac) * self.n_samples)\n",
        "        self.Xtr, self.Xva = self.full[idx[:cut]], self.full[idx[cut:]]\n",
        "\n",
        "        Scorer = CopulaBIC if str(score_type).lower() == \"copula\" else GaussianBIC\n",
        "        self.scorer_cls = Scorer\n",
        "        self.bic_va = Scorer(self.Xva)\n",
        "\n",
        "        self.state_space_shape = (self.n_nodes * self.n_nodes,)\n",
        "        self.n_actions = 3 * self.n_nodes * (self.n_nodes - 1)\n",
        "        self.action_map = self._create_action_map()\n",
        "\n",
        "        self.current_adj = np.zeros((self.n_nodes, self.n_nodes), dtype=np.float64)\n",
        "        self.max_steps = 10 * self.n_nodes\n",
        "        self.current_step = 0\n",
        "\n",
        "        self.edge_budget = int(max(1, edge_budget_ratio * self.n_nodes))\n",
        "        self.lambda_l1 = float(lambda_l1)\n",
        "        self.action_cost = float(action_cost)\n",
        "\n",
        "        self._warm_adj = None\n",
        "        if warm_start:\n",
        "            print(\"\\n[Warm-start] searching ...\")\n",
        "            self._warm_adj = warm_start_greedy_bic(\n",
        "                self.Xtr, self.Xva,\n",
        "                edge_budget=self.edge_budget,\n",
        "                scorer_cls=self.scorer_cls,\n",
        "                max_passes=5, topk_per_pass=5, restarts=5, seed=SEED\n",
        "            )\n",
        "            print(\"[Warm-start] edges:\", int(np.sum(self._warm_adj)))\n",
        "\n",
        "    def _create_action_map(self):\n",
        "        mapping, idx = {}, 0\n",
        "        for i in range(self.n_nodes):\n",
        "            for j in range(self.n_nodes):\n",
        "                if i == j: continue\n",
        "                mapping[idx] = (\"add\", i, j); idx += 1\n",
        "                mapping[idx] = (\"remove\", i, j); idx += 1\n",
        "                mapping[idx] = (\"reverse\", i, j); idx += 1\n",
        "        return mapping\n",
        "\n",
        "    def _val_bic(self, A): return self.bic_va.bic(A)\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_step = 0\n",
        "        if self._warm_adj is not None: self.current_adj = self._warm_adj.copy()\n",
        "        else: self.current_adj[:] = 0.0\n",
        "        return self.current_adj.flatten().copy()\n",
        "\n",
        "    def step(self, action_idx):\n",
        "        op, i, j = self.action_map[action_idx]\n",
        "        prev_adj = self.current_adj.copy()\n",
        "\n",
        "        if op == \"add\" and np.sum(self.current_adj) >= self.edge_budget:\n",
        "            self.current_step += 1\n",
        "            return self.current_adj.flatten(), -0.2, self.current_step >= self.max_steps, {}\n",
        "\n",
        "        trial = self.current_adj.copy()\n",
        "        if op == \"add\":\n",
        "            trial[i, j] = 1.0\n",
        "        elif op == \"remove\":\n",
        "            trial[i, j] = 0.0\n",
        "        elif op == \"reverse\":\n",
        "            if self.current_adj[i, j] == 1.0:\n",
        "                trial[i, j] = 0.0; trial[j, i] = 1.0\n",
        "            else:\n",
        "                self.current_step += 1\n",
        "                return self.current_adj.flatten(), -0.2, self.current_step >= self.max_steps, {}\n",
        "        np.fill_diagonal(trial, 0.0)\n",
        "\n",
        "        if not nx.is_directed_acyclic_graph(nx.DiGraph(trial)):\n",
        "            self.current_step += 1\n",
        "            return self.current_adj.flatten(), -0.5, self.current_step >= self.max_steps, {}\n",
        "\n",
        "        self.current_adj = trial\n",
        "        self.current_step += 1\n",
        "        r = self._reward(prev_adj, self.current_adj)\n",
        "        done = self.current_step >= self.max_steps\n",
        "        return self.current_adj.flatten().copy(), r, done, {}\n",
        "\n",
        "    def _reward(self, prev_adj, new_adj):\n",
        "        prev = self._val_bic(prev_adj)\n",
        "        new  = self._val_bic(new_adj)\n",
        "        score = (new - prev) / max(self.n_nodes, 1)\n",
        "        score = float(np.clip(score, -100.0, 10.0))\n",
        "        step_pen = -0.002\n",
        "        sparsity = - self.lambda_l1 * float(np.sum(new_adj))\n",
        "        act_pen = - self.action_cost if (new_adj != prev_adj).any() else 0.0\n",
        "        total = score + sparsity + act_pen + step_pen\n",
        "        return float(np.clip(total, -100.0, 20.0))\n",
        "\n",
        "\n",
        "# -------------------- Agent (Double DQN + Polyak) --------------------\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(state_size, 256, dtype=DTYPE),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 256, dtype=DTYPE),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, action_size, dtype=DTYPE),\n",
        "        )\n",
        "    def forward(self, x): return self.layers(x)\n",
        "\n",
        "class CausalAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.action_size = action_size\n",
        "        self.q = QNetwork(state_size, action_size).to(device)\n",
        "        self.t = QNetwork(state_size, action_size).to(device)\n",
        "        self.t.load_state_dict(self.q.state_dict())\n",
        "        self.tau = 0.005\n",
        "        self.opt = optim.Adam(self.q.parameters(), lr=5e-4)\n",
        "        self.gamma = 0.95\n",
        "\n",
        "        self.eps_start, self.eps_end = 1.0, 0.05\n",
        "        self.eps_decay_steps = 250_000\n",
        "        self.total_steps = 0\n",
        "        self.epsilon = self.eps_start\n",
        "\n",
        "        self.mem = deque(maxlen=120_000)\n",
        "        self.batch = 256\n",
        "\n",
        "    def _update_eps(self):\n",
        "        self.total_steps += 1\n",
        "        frac = min(1.0, self.total_steps / self.eps_decay_steps)\n",
        "        self.epsilon = self.eps_start + frac * (self.eps_end - self.eps_start)\n",
        "\n",
        "    def remember(self, s, a, r, ns, d): self.mem.append((s, a, r, ns, d))\n",
        "\n",
        "    def act(self, state):\n",
        "        self._update_eps()\n",
        "        if random.random() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        st = torch.tensor(state, dtype=DTYPE, device=device).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            qv = self.q(st)\n",
        "        return int(qv.argmax(dim=1).item())\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.mem) < self.batch: return\n",
        "        batch = random.sample(self.mem, self.batch)\n",
        "        s  = torch.tensor(np.array([e[0] for e in batch]), dtype=DTYPE, device=device)\n",
        "        a  = torch.tensor([e[1] for e in batch], dtype=torch.long, device=device).unsqueeze(1)\n",
        "        r  = torch.tensor([e[2] for e in batch], dtype=DTYPE, device=device).unsqueeze(1)\n",
        "        ns = torch.tensor(np.array([e[3] for e in batch]), dtype=DTYPE, device=device)\n",
        "        d  = torch.tensor([e[4] for e in batch], dtype=DTYPE, device=device).unsqueeze(1)\n",
        "\n",
        "        q_sa = self.q(s).gather(1, a)\n",
        "        with torch.no_grad():\n",
        "            na_online = self.q(ns).argmax(1, keepdim=True)\n",
        "            q_next = self.t(ns).gather(1, na_online)\n",
        "            target = r + (1.0 - d) * self.gamma * q_next\n",
        "\n",
        "        loss = nn.MSELoss()(q_sa, target)\n",
        "        self.opt.zero_grad(); loss.backward()\n",
        "        nn.utils.clip_grad_norm_(self.q.parameters(), 5.0)\n",
        "        self.opt.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for tparam, qparam in zip(self.t.parameters(), self.q.parameters()):\n",
        "                tparam.data.mul_(1 - self.tau).add_(self.tau * qparam.data)\n",
        "\n",
        "\n",
        "# -------------------- CAM pruning --------------------\n",
        "def graph_prunned_by_coef(parent_mat, X, th=CAM_TH):\n",
        "    \"\"\"\n",
        "    parent_mat[i, j]=1 means j -> i (parents per row).\n",
        "    Returns binary parent matrix via linear regression thresholding.\n",
        "    \"\"\"\n",
        "    d = parent_mat.shape[0]\n",
        "    reg = LinearRegression()\n",
        "    W = []\n",
        "    for i in range(d):\n",
        "        col = np.abs(parent_mat[i]) > 0.1\n",
        "        if np.sum(col) <= 0:\n",
        "            W.append(np.zeros(d)); continue\n",
        "        X_train = X[:, col]\n",
        "        y = X[:, i]\n",
        "        reg.fit(X_train, y)\n",
        "        coeff = reg.coef_\n",
        "        new_coeff = np.zeros(d)\n",
        "        cj = 0\n",
        "        for ci in range(d):\n",
        "            if col[ci]:\n",
        "                new_coeff[ci] = coeff[cj]; cj += 1\n",
        "        W.append(new_coeff)\n",
        "    return (np.abs(np.vstack(W)) > th).astype(np.float64)\n",
        "\n",
        "def cam_prune_linear_from_A(A_directed, X, th=CAM_TH):\n",
        "    # Our A: i->j. Convert to parent matrix then back.\n",
        "    parents = A_directed.T\n",
        "    pruned_parents = graph_prunned_by_coef(parents, X, th=th)\n",
        "    return pruned_parents.T\n",
        "\n",
        "\n",
        "# -------------------- Main --------------------\n",
        "if __name__ == \"__main__\":\n",
        "    X = load_data(DATA_CSV)\n",
        "    if X is None: raise SystemExit\n",
        "    p = X.shape[1]\n",
        "    GT = load_truth(GT_NPY, p=p)\n",
        "\n",
        "    # Opponent\n",
        "    Gdag = get_grandag_adj(X, iterations=G_ITER)\n",
        "    if Gdag.shape != (p, p):\n",
        "        Gdag = Gdag[:p, :p]\n",
        "    Gdag_bin = binarize(Gdag)\n",
        "\n",
        "    # Env + Agent\n",
        "    env = CausalDiscoveryEnv(\n",
        "        X, Gdag,\n",
        "        val_frac=0.2,\n",
        "        edge_budget_ratio=EDGE_BUDGET_RATIO,\n",
        "        lambda_l1=LAMBDA_L1, action_cost=ACTION_COST,\n",
        "        warm_start=True,\n",
        "        score_type=SCORE_TYPE  # \"copula\" or \"gaussian\"\n",
        "    )\n",
        "    agent = CausalAgent(state_size=env.state_space_shape[0], action_size=env.n_actions)\n",
        "\n",
        "    # Track best snapshots\n",
        "    best_valbic = -1e18\n",
        "    best_agent_adj = None\n",
        "    best_tpr = -1.0\n",
        "    best_agent_adj_by_tpr = None\n",
        "\n",
        "    # Training loop\n",
        "    print(\"\\nStarting training...\")\n",
        "    for ep in range(N_EPISODES):\n",
        "        s = env.reset()\n",
        "        total = 0.0; done = False\n",
        "        while not done:\n",
        "            a = agent.act(s)\n",
        "            ns, r, done, _ = env.step(a)\n",
        "            agent.remember(s, a, r, ns, done)\n",
        "            s = ns; total += r\n",
        "            agent.replay()\n",
        "\n",
        "        msg = f\"Ep {ep+1:04d}/{N_EPISODES}  Reward={total:8.3f}  eps={agent.epsilon:.3f}\"\n",
        "\n",
        "        if (ep + 1) % EVAL_EVERY == 0:\n",
        "            A_now = binarize(env.current_adj)\n",
        "            valBIC_agent = env._val_bic(A_now)\n",
        "            valBIC_gran  = env._val_bic(Gdag_bin)\n",
        "\n",
        "            # keep best-by-ValBIC (no GT)\n",
        "            if valBIC_agent > best_valbic:\n",
        "                best_valbic = valBIC_agent\n",
        "                best_agent_adj = A_now.copy()\n",
        "\n",
        "            # CAM (eval only)\n",
        "            A_cam = cam_prune_linear_from_A(A_now, X, th=CAM_TH).astype(int)\n",
        "\n",
        "            msg += f\" | ValBIC(A)={valBIC_agent:.1f}  ValBIC(G)={valBIC_gran:.1f}\"\n",
        "\n",
        "            if GT is not None and GT.shape == A_now.shape:\n",
        "                met_A  = eval_against_gt(A_now, GT)\n",
        "                met_AC = eval_against_gt(A_cam, GT)\n",
        "                met_G  = eval_against_gt(Gdag_bin, GT)\n",
        "\n",
        "                # keep best-by-TPR snapshot (analysis only)\n",
        "                if met_A[\"tpr\"] > best_tpr:\n",
        "                    best_tpr = met_A[\"tpr\"]\n",
        "                    best_agent_adj_by_tpr = A_now.copy()\n",
        "\n",
        "                msg += (f\"\\n   A(raw): {met_A}\"\n",
        "                        f\"\\n   A+CAM: {met_AC}\"\n",
        "                        f\"\\n   G-DAG: {met_G}\")\n",
        "        print(msg)\n",
        "\n",
        "    print(\"\\nTraining finished.\")\n",
        "    A_final = binarize(env.current_adj)\n",
        "    A_cam_final = cam_prune_linear_from_A(A_final, X, th=CAM_TH).astype(int)\n",
        "\n",
        "    # Final reporting\n",
        "    print(\"\\n--- Final (Validation) ---\")\n",
        "    print(\"ValBIC(agent):    \", env._val_bic(A_final))\n",
        "    print(\"ValBIC(agent+CAM):\", env._val_bic(A_cam_final))\n",
        "    print(\"ValBIC(GraN-DAG): \", env._val_bic(Gdag_bin))\n",
        "\n",
        "    if GT is not None and GT.shape == A_final.shape:\n",
        "        print(\"\\n--- Final (GT) Metrics ---\")\n",
        "        print(\"Agent (raw): \", eval_against_gt(A_final, GT))\n",
        "        print(\"Agent+CAM:  \", eval_against_gt(A_cam_final, GT))\n",
        "        print(\"GraN-DAG:   \", eval_against_gt(Gdag_bin, GT))\n",
        "\n",
        "        if best_agent_adj is not None:\n",
        "            print(\"\\n--- Best-by-Validation snapshot ---\")\n",
        "            print(\"A(best ValBIC): \", eval_against_gt(best_agent_adj, GT))\n",
        "\n",
        "        if best_agent_adj_by_tpr is not None:\n",
        "            print(\"\\n--- Best-by-TPR snapshot (analysis only) ---\")\n",
        "            print(f\"Best TPR = {best_tpr:.4f}\")\n",
        "            print(\"A(best TPR): \", eval_against_gt(best_agent_adj_by_tpr, GT))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fuZdPZ8Ugx7q",
        "outputId": "ec3e9018-5b42-46a1-f866-3100a1945320"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded data: 10000 samples, 37 vars\n",
            "Loaded ground truth: (37, 37)\n",
            "\n",
            "Running GraN-DAG (iterations=1000)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iterations: 100%|██████████| 1000/1000 [00:26<00:00, 37.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GraN-DAG done.\n",
            "\n",
            "[Warm-start] searching ...\n",
            "[Warm-start] edges: 15\n",
            "\n",
            "Starting training...\n",
            "Ep 0001/400  Reward=-218.937  eps=0.999\n",
            "Ep 0002/400  Reward=-162.700  eps=0.997\n",
            "Ep 0003/400  Reward=-198.243  eps=0.996\n",
            "Ep 0004/400  Reward=-177.633  eps=0.994\n",
            "Ep 0005/400  Reward=-240.795  eps=0.993\n",
            "Ep 0006/400  Reward=-201.221  eps=0.992\n",
            "Ep 0007/400  Reward=-203.703  eps=0.990\n",
            "Ep 0008/400  Reward=-175.785  eps=0.989\n",
            "Ep 0009/400  Reward=-207.475  eps=0.987\n",
            "Ep 0010/400  Reward=-157.953  eps=0.986\n",
            "Ep 0011/400  Reward=-139.005  eps=0.985\n",
            "Ep 0012/400  Reward=-194.031  eps=0.983\n",
            "Ep 0013/400  Reward=-139.522  eps=0.982\n",
            "Ep 0014/400  Reward=-204.042  eps=0.980\n",
            "Ep 0015/400  Reward=-205.354  eps=0.979\n",
            "Ep 0016/400  Reward=-214.088  eps=0.978\n",
            "Ep 0017/400  Reward=-165.958  eps=0.976\n",
            "Ep 0018/400  Reward=-230.213  eps=0.975\n",
            "Ep 0019/400  Reward=-224.088  eps=0.973\n",
            "Ep 0020/400  Reward=-199.901  eps=0.972 | ValBIC(A)=-193334.6  ValBIC(G)=-203003.2\n",
            "   A(raw): {'total_edges_original': 46, 'total_edges_predicted': 40, 'correct_edges': 7, 'fdr': 0.825, 'tpr': 0.1522, 'fpr': 0.0249, 'shd': 68, 'nnz': 40}\n",
            "   A+CAM: {'total_edges_original': 46, 'total_edges_predicted': 15, 'correct_edges': 7, 'fdr': 0.5333, 'tpr': 0.1522, 'fpr': 0.006, 'shd': 43, 'nnz': 15}\n",
            "   G-DAG: {'total_edges_original': 46, 'total_edges_predicted': 1, 'correct_edges': 1, 'fdr': 0.0, 'tpr': 0.0217, 'fpr': 0.0, 'shd': 45, 'nnz': 1}\n",
            "Ep 0021/400  Reward=-188.767  eps=0.970\n",
            "Ep 0022/400  Reward=-233.687  eps=0.969\n",
            "Ep 0023/400  Reward=-184.665  eps=0.968\n",
            "Ep 0024/400  Reward=-168.046  eps=0.966\n",
            "Ep 0025/400  Reward=-208.196  eps=0.965\n",
            "Ep 0026/400  Reward=-159.316  eps=0.963\n",
            "Ep 0027/400  Reward=-153.887  eps=0.962\n",
            "Ep 0028/400  Reward=-146.683  eps=0.961\n",
            "Ep 0029/400  Reward=-169.649  eps=0.959\n",
            "Ep 0030/400  Reward=-264.843  eps=0.958\n",
            "Ep 0031/400  Reward=-202.174  eps=0.956\n",
            "Ep 0032/400  Reward=-184.042  eps=0.955\n",
            "Ep 0033/400  Reward=-188.400  eps=0.954\n",
            "Ep 0034/400  Reward=-233.415  eps=0.952\n",
            "Ep 0035/400  Reward=-171.622  eps=0.951\n",
            "Ep 0036/400  Reward=-192.465  eps=0.949\n",
            "Ep 0037/400  Reward=-187.047  eps=0.948\n",
            "Ep 0038/400  Reward=-165.906  eps=0.947\n",
            "Ep 0039/400  Reward=-199.667  eps=0.945\n",
            "Ep 0040/400  Reward=-169.424  eps=0.944 | ValBIC(A)=-192537.2  ValBIC(G)=-203003.2\n",
            "   A(raw): {'total_edges_original': 46, 'total_edges_predicted': 40, 'correct_edges': 10, 'fdr': 0.75, 'tpr': 0.2174, 'fpr': 0.0227, 'shd': 64, 'nnz': 40}\n",
            "   A+CAM: {'total_edges_original': 46, 'total_edges_predicted': 15, 'correct_edges': 10, 'fdr': 0.3333, 'tpr': 0.2174, 'fpr': 0.0038, 'shd': 39, 'nnz': 15}\n",
            "   G-DAG: {'total_edges_original': 46, 'total_edges_predicted': 1, 'correct_edges': 1, 'fdr': 0.0, 'tpr': 0.0217, 'fpr': 0.0, 'shd': 45, 'nnz': 1}\n",
            "Ep 0041/400  Reward=-153.800  eps=0.942\n",
            "Ep 0042/400  Reward=-198.035  eps=0.941\n",
            "Ep 0043/400  Reward=-175.057  eps=0.940\n",
            "Ep 0044/400  Reward=-229.216  eps=0.938\n",
            "Ep 0045/400  Reward=-201.128  eps=0.937\n",
            "Ep 0046/400  Reward=-177.236  eps=0.935\n",
            "Ep 0047/400  Reward=-222.807  eps=0.934\n",
            "Ep 0048/400  Reward=-234.487  eps=0.933\n",
            "Ep 0049/400  Reward=-213.871  eps=0.931\n",
            "Ep 0050/400  Reward=-168.135  eps=0.930\n",
            "Ep 0051/400  Reward=-161.782  eps=0.928\n",
            "Ep 0052/400  Reward=-207.044  eps=0.927\n",
            "Ep 0053/400  Reward=-196.615  eps=0.925\n",
            "Ep 0054/400  Reward=-229.375  eps=0.924\n",
            "Ep 0055/400  Reward=-177.497  eps=0.923\n",
            "Ep 0056/400  Reward=-220.209  eps=0.921\n",
            "Ep 0057/400  Reward=-152.198  eps=0.920\n",
            "Ep 0058/400  Reward=-177.022  eps=0.918\n",
            "Ep 0059/400  Reward=-197.504  eps=0.917\n",
            "Ep 0060/400  Reward=-191.533  eps=0.916 | ValBIC(A)=-193299.4  ValBIC(G)=-203003.2\n",
            "   A(raw): {'total_edges_original': 46, 'total_edges_predicted': 40, 'correct_edges': 8, 'fdr': 0.8, 'tpr': 0.1739, 'fpr': 0.0242, 'shd': 63, 'nnz': 40}\n",
            "   A+CAM: {'total_edges_original': 46, 'total_edges_predicted': 15, 'correct_edges': 7, 'fdr': 0.5333, 'tpr': 0.1522, 'fpr': 0.006, 'shd': 42, 'nnz': 15}\n",
            "   G-DAG: {'total_edges_original': 46, 'total_edges_predicted': 1, 'correct_edges': 1, 'fdr': 0.0, 'tpr': 0.0217, 'fpr': 0.0, 'shd': 45, 'nnz': 1}\n",
            "Ep 0061/400  Reward=-196.074  eps=0.914\n",
            "Ep 0062/400  Reward=-203.459  eps=0.913\n",
            "Ep 0063/400  Reward=-176.961  eps=0.911\n",
            "Ep 0064/400  Reward=-193.358  eps=0.910\n",
            "Ep 0065/400  Reward=-167.789  eps=0.909\n",
            "Ep 0066/400  Reward=-170.026  eps=0.907\n",
            "Ep 0067/400  Reward=-244.585  eps=0.906\n",
            "Ep 0068/400  Reward=-158.461  eps=0.904\n",
            "Ep 0069/400  Reward=-207.518  eps=0.903\n",
            "Ep 0070/400  Reward=-192.257  eps=0.902\n",
            "Ep 0071/400  Reward=-211.458  eps=0.900\n",
            "Ep 0072/400  Reward=-218.971  eps=0.899\n",
            "Ep 0073/400  Reward=-193.735  eps=0.897\n",
            "Ep 0074/400  Reward=-200.219  eps=0.896\n",
            "Ep 0075/400  Reward=-144.343  eps=0.895\n",
            "Ep 0076/400  Reward=-175.166  eps=0.893\n",
            "Ep 0077/400  Reward=-196.600  eps=0.892\n",
            "Ep 0078/400  Reward=-243.673  eps=0.890\n",
            "Ep 0079/400  Reward=-193.295  eps=0.889\n",
            "Ep 0080/400  Reward=-210.511  eps=0.888 | ValBIC(A)=-194269.8  ValBIC(G)=-203003.2\n",
            "   A(raw): {'total_edges_original': 46, 'total_edges_predicted': 40, 'correct_edges': 9, 'fdr': 0.775, 'tpr': 0.1957, 'fpr': 0.0234, 'shd': 63, 'nnz': 40}\n",
            "   A+CAM: {'total_edges_original': 46, 'total_edges_predicted': 17, 'correct_edges': 8, 'fdr': 0.5294, 'tpr': 0.1739, 'fpr': 0.0068, 'shd': 42, 'nnz': 17}\n",
            "   G-DAG: {'total_edges_original': 46, 'total_edges_predicted': 1, 'correct_edges': 1, 'fdr': 0.0, 'tpr': 0.0217, 'fpr': 0.0, 'shd': 45, 'nnz': 1}\n",
            "Ep 0081/400  Reward=-222.993  eps=0.886\n",
            "Ep 0082/400  Reward=-197.223  eps=0.885\n",
            "Ep 0083/400  Reward=-179.800  eps=0.883\n",
            "Ep 0084/400  Reward=-236.416  eps=0.882\n",
            "Ep 0085/400  Reward=-193.684  eps=0.880\n",
            "Ep 0086/400  Reward=-215.890  eps=0.879\n",
            "Ep 0087/400  Reward=-161.321  eps=0.878\n",
            "Ep 0088/400  Reward=-197.703  eps=0.876\n",
            "Ep 0089/400  Reward=-208.326  eps=0.875\n",
            "Ep 0090/400  Reward=-149.807  eps=0.873\n",
            "Ep 0091/400  Reward=-260.766  eps=0.872\n",
            "Ep 0092/400  Reward=-186.017  eps=0.871\n",
            "Ep 0093/400  Reward=-174.349  eps=0.869\n",
            "Ep 0094/400  Reward=-178.902  eps=0.868\n",
            "Ep 0095/400  Reward=-201.424  eps=0.866\n",
            "Ep 0096/400  Reward=-205.277  eps=0.865\n",
            "Ep 0097/400  Reward=-195.377  eps=0.864\n",
            "Ep 0098/400  Reward=-189.177  eps=0.862\n",
            "Ep 0099/400  Reward=-204.003  eps=0.861\n",
            "Ep 0100/400  Reward=-172.466  eps=0.859 | ValBIC(A)=-192975.7  ValBIC(G)=-203003.2\n",
            "   A(raw): {'total_edges_original': 46, 'total_edges_predicted': 40, 'correct_edges': 7, 'fdr': 0.825, 'tpr': 0.1522, 'fpr': 0.0249, 'shd': 67, 'nnz': 40}\n",
            "   A+CAM: {'total_edges_original': 46, 'total_edges_predicted': 16, 'correct_edges': 7, 'fdr': 0.5625, 'tpr': 0.1522, 'fpr': 0.0068, 'shd': 44, 'nnz': 16}\n",
            "   G-DAG: {'total_edges_original': 46, 'total_edges_predicted': 1, 'correct_edges': 1, 'fdr': 0.0, 'tpr': 0.0217, 'fpr': 0.0, 'shd': 45, 'nnz': 1}\n",
            "Ep 0101/400  Reward=-180.618  eps=0.858\n",
            "Ep 0102/400  Reward=-204.559  eps=0.857\n",
            "Ep 0103/400  Reward=-161.186  eps=0.855\n",
            "Ep 0104/400  Reward=-228.595  eps=0.854\n",
            "Ep 0105/400  Reward=-213.916  eps=0.852\n",
            "Ep 0106/400  Reward=-264.003  eps=0.851\n",
            "Ep 0107/400  Reward=-202.197  eps=0.850\n",
            "Ep 0108/400  Reward=-215.777  eps=0.848\n",
            "Ep 0109/400  Reward=-232.298  eps=0.847\n",
            "Ep 0110/400  Reward=-227.390  eps=0.845\n",
            "Ep 0111/400  Reward=-206.881  eps=0.844\n",
            "Ep 0112/400  Reward=-207.780  eps=0.843\n",
            "Ep 0113/400  Reward=-218.358  eps=0.841\n",
            "Ep 0114/400  Reward=-189.390  eps=0.840\n",
            "Ep 0115/400  Reward=-166.364  eps=0.838\n",
            "Ep 0116/400  Reward=-243.250  eps=0.837\n",
            "Ep 0117/400  Reward=-192.403  eps=0.835\n",
            "Ep 0118/400  Reward=-175.054  eps=0.834\n",
            "Ep 0119/400  Reward=-169.878  eps=0.833\n",
            "Ep 0120/400  Reward=-193.403  eps=0.831 | ValBIC(A)=-192565.6  ValBIC(G)=-203003.2\n",
            "   A(raw): {'total_edges_original': 46, 'total_edges_predicted': 40, 'correct_edges': 7, 'fdr': 0.825, 'tpr': 0.1522, 'fpr': 0.0249, 'shd': 67, 'nnz': 40}\n",
            "   A+CAM: {'total_edges_original': 46, 'total_edges_predicted': 18, 'correct_edges': 7, 'fdr': 0.6111, 'tpr': 0.1522, 'fpr': 0.0083, 'shd': 45, 'nnz': 18}\n",
            "   G-DAG: {'total_edges_original': 46, 'total_edges_predicted': 1, 'correct_edges': 1, 'fdr': 0.0, 'tpr': 0.0217, 'fpr': 0.0, 'shd': 45, 'nnz': 1}\n",
            "Ep 0121/400  Reward=-194.117  eps=0.830\n",
            "Ep 0122/400  Reward=-150.572  eps=0.828\n",
            "Ep 0123/400  Reward=-167.737  eps=0.827\n",
            "Ep 0124/400  Reward=-153.564  eps=0.826\n",
            "Ep 0125/400  Reward=-179.806  eps=0.824\n",
            "Ep 0126/400  Reward=-147.567  eps=0.823\n",
            "Ep 0127/400  Reward=-219.502  eps=0.821\n",
            "Ep 0128/400  Reward=-177.726  eps=0.820\n",
            "Ep 0129/400  Reward=-201.824  eps=0.819\n",
            "Ep 0130/400  Reward=-215.158  eps=0.817\n",
            "Ep 0131/400  Reward=-178.861  eps=0.816\n",
            "Ep 0132/400  Reward=-175.723  eps=0.814\n",
            "Ep 0133/400  Reward=-139.232  eps=0.813\n",
            "Ep 0134/400  Reward=-160.625  eps=0.812\n",
            "Ep 0135/400  Reward=-162.777  eps=0.810\n",
            "Ep 0136/400  Reward=-179.338  eps=0.809\n",
            "Ep 0137/400  Reward=-183.016  eps=0.807\n",
            "Ep 0138/400  Reward=-217.624  eps=0.806\n",
            "Ep 0139/400  Reward=-218.462  eps=0.805\n",
            "Ep 0140/400  Reward=-188.413  eps=0.803 | ValBIC(A)=-192824.8  ValBIC(G)=-203003.2\n",
            "   A(raw): {'total_edges_original': 46, 'total_edges_predicted': 40, 'correct_edges': 8, 'fdr': 0.8, 'tpr': 0.1739, 'fpr': 0.0242, 'shd': 67, 'nnz': 40}\n",
            "   A+CAM: {'total_edges_original': 46, 'total_edges_predicted': 16, 'correct_edges': 8, 'fdr': 0.5, 'tpr': 0.1739, 'fpr': 0.006, 'shd': 43, 'nnz': 16}\n",
            "   G-DAG: {'total_edges_original': 46, 'total_edges_predicted': 1, 'correct_edges': 1, 'fdr': 0.0, 'tpr': 0.0217, 'fpr': 0.0, 'shd': 45, 'nnz': 1}\n",
            "Ep 0141/400  Reward=-216.537  eps=0.802\n",
            "Ep 0142/400  Reward=-182.102  eps=0.800\n",
            "Ep 0143/400  Reward=-173.934  eps=0.799\n",
            "Ep 0144/400  Reward=-177.431  eps=0.798\n",
            "Ep 0145/400  Reward=-229.737  eps=0.796\n",
            "Ep 0146/400  Reward=-237.598  eps=0.795\n",
            "Ep 0147/400  Reward=-190.678  eps=0.793\n",
            "Ep 0148/400  Reward=-171.039  eps=0.792\n",
            "Ep 0149/400  Reward=-165.612  eps=0.791\n",
            "Ep 0150/400  Reward=-189.270  eps=0.789\n",
            "Ep 0151/400  Reward=-223.967  eps=0.788\n",
            "Ep 0152/400  Reward=-192.195  eps=0.786\n",
            "Ep 0153/400  Reward=-196.815  eps=0.785\n",
            "Ep 0154/400  Reward=-185.075  eps=0.783\n",
            "Ep 0155/400  Reward=-167.251  eps=0.782\n",
            "Ep 0156/400  Reward=-188.264  eps=0.781\n",
            "Ep 0157/400  Reward=-169.587  eps=0.779\n",
            "Ep 0158/400  Reward=-156.814  eps=0.778\n",
            "Ep 0159/400  Reward=-243.155  eps=0.776\n",
            "Ep 0160/400  Reward=-174.769  eps=0.775 | ValBIC(A)=-193109.5  ValBIC(G)=-203003.2\n",
            "   A(raw): {'total_edges_original': 46, 'total_edges_predicted': 40, 'correct_edges': 7, 'fdr': 0.825, 'tpr': 0.1522, 'fpr': 0.0249, 'shd': 64, 'nnz': 40}\n",
            "   A+CAM: {'total_edges_original': 46, 'total_edges_predicted': 14, 'correct_edges': 5, 'fdr': 0.6429, 'tpr': 0.1087, 'fpr': 0.0068, 'shd': 43, 'nnz': 14}\n",
            "   G-DAG: {'total_edges_original': 46, 'total_edges_predicted': 1, 'correct_edges': 1, 'fdr': 0.0, 'tpr': 0.0217, 'fpr': 0.0, 'shd': 45, 'nnz': 1}\n",
            "Ep 0161/400  Reward=-204.914  eps=0.774\n",
            "Ep 0162/400  Reward=-163.482  eps=0.772\n",
            "Ep 0163/400  Reward=-200.396  eps=0.771\n",
            "Ep 0164/400  Reward=-187.728  eps=0.769\n",
            "Ep 0165/400  Reward=-201.390  eps=0.768\n",
            "Ep 0166/400  Reward=-167.099  eps=0.767\n",
            "Ep 0167/400  Reward=-136.394  eps=0.765\n",
            "Ep 0168/400  Reward=-147.213  eps=0.764\n",
            "Ep 0169/400  Reward=-172.368  eps=0.762\n",
            "Ep 0170/400  Reward=-241.492  eps=0.761\n",
            "Ep 0171/400  Reward=-288.739  eps=0.760\n",
            "Ep 0172/400  Reward=-200.955  eps=0.758\n",
            "Ep 0173/400  Reward=-252.841  eps=0.757\n",
            "Ep 0174/400  Reward=-185.345  eps=0.755\n",
            "Ep 0175/400  Reward=-208.040  eps=0.754\n",
            "Ep 0176/400  Reward=-163.232  eps=0.753\n",
            "Ep 0177/400  Reward=-144.121  eps=0.751\n",
            "Ep 0178/400  Reward=-263.879  eps=0.750\n",
            "Ep 0179/400  Reward=-193.868  eps=0.748\n",
            "Ep 0180/400  Reward=-164.529  eps=0.747 | ValBIC(A)=-192541.4  ValBIC(G)=-203003.2\n",
            "   A(raw): {'total_edges_original': 46, 'total_edges_predicted': 40, 'correct_edges': 9, 'fdr': 0.775, 'tpr': 0.1957, 'fpr': 0.0234, 'shd': 64, 'nnz': 40}\n",
            "   A+CAM: {'total_edges_original': 46, 'total_edges_predicted': 17, 'correct_edges': 9, 'fdr': 0.4706, 'tpr': 0.1957, 'fpr': 0.006, 'shd': 41, 'nnz': 17}\n",
            "   G-DAG: {'total_edges_original': 46, 'total_edges_predicted': 1, 'correct_edges': 1, 'fdr': 0.0, 'tpr': 0.0217, 'fpr': 0.0, 'shd': 45, 'nnz': 1}\n",
            "Ep 0181/400  Reward=-173.692  eps=0.746\n",
            "Ep 0182/400  Reward=-166.343  eps=0.744\n",
            "Ep 0183/400  Reward=-150.694  eps=0.743\n",
            "Ep 0184/400  Reward=-155.336  eps=0.741\n",
            "Ep 0185/400  Reward=-139.031  eps=0.740\n",
            "Ep 0186/400  Reward=-185.636  eps=0.738\n",
            "Ep 0187/400  Reward=-197.698  eps=0.737\n",
            "Ep 0188/400  Reward=-206.732  eps=0.736\n",
            "Ep 0189/400  Reward=-204.354  eps=0.734\n",
            "Ep 0190/400  Reward=-148.872  eps=0.733\n",
            "Ep 0191/400  Reward=-244.383  eps=0.731\n",
            "Ep 0192/400  Reward=-272.802  eps=0.730\n",
            "Ep 0193/400  Reward=-171.059  eps=0.729\n",
            "Ep 0194/400  Reward=-181.637  eps=0.727\n",
            "Ep 0195/400  Reward=-187.145  eps=0.726\n",
            "Ep 0196/400  Reward=-219.277  eps=0.724\n",
            "Ep 0197/400  Reward=-146.748  eps=0.723\n",
            "Ep 0198/400  Reward=-227.353  eps=0.722\n",
            "Ep 0199/400  Reward=-224.167  eps=0.720\n",
            "Ep 0200/400  Reward=-166.003  eps=0.719 | ValBIC(A)=-192522.2  ValBIC(G)=-203003.2\n",
            "   A(raw): {'total_edges_original': 46, 'total_edges_predicted': 40, 'correct_edges': 9, 'fdr': 0.775, 'tpr': 0.1957, 'fpr': 0.0234, 'shd': 64, 'nnz': 40}\n",
            "   A+CAM: {'total_edges_original': 46, 'total_edges_predicted': 15, 'correct_edges': 9, 'fdr': 0.4, 'tpr': 0.1957, 'fpr': 0.0045, 'shd': 40, 'nnz': 15}\n",
            "   G-DAG: {'total_edges_original': 46, 'total_edges_predicted': 1, 'correct_edges': 1, 'fdr': 0.0, 'tpr': 0.0217, 'fpr': 0.0, 'shd': 45, 'nnz': 1}\n",
            "Ep 0201/400  Reward=-216.075  eps=0.717\n",
            "Ep 0202/400  Reward=-155.613  eps=0.716\n",
            "Ep 0203/400  Reward=-193.092  eps=0.715\n",
            "Ep 0204/400  Reward=-219.288  eps=0.713\n",
            "Ep 0205/400  Reward=-147.213  eps=0.712\n",
            "Ep 0206/400  Reward=-242.227  eps=0.710\n",
            "Ep 0207/400  Reward=-170.745  eps=0.709\n",
            "Ep 0208/400  Reward=-182.489  eps=0.708\n",
            "Ep 0209/400  Reward=-223.659  eps=0.706\n",
            "Ep 0210/400  Reward=-247.656  eps=0.705\n",
            "Ep 0211/400  Reward=-190.157  eps=0.703\n",
            "Ep 0212/400  Reward=-150.672  eps=0.702\n",
            "Ep 0213/400  Reward=-224.009  eps=0.701\n",
            "Ep 0214/400  Reward=-173.112  eps=0.699\n",
            "Ep 0215/400  Reward=-208.399  eps=0.698\n",
            "Ep 0216/400  Reward=-150.190  eps=0.696\n",
            "Ep 0217/400  Reward=-195.548  eps=0.695\n",
            "Ep 0218/400  Reward=-216.621  eps=0.693\n",
            "Ep 0219/400  Reward=-177.848  eps=0.692\n",
            "Ep 0220/400  Reward=-171.378  eps=0.691 | ValBIC(A)=-193005.8  ValBIC(G)=-203003.2\n",
            "   A(raw): {'total_edges_original': 46, 'total_edges_predicted': 40, 'correct_edges': 8, 'fdr': 0.8, 'tpr': 0.1739, 'fpr': 0.0242, 'shd': 64, 'nnz': 40}\n",
            "   A+CAM: {'total_edges_original': 46, 'total_edges_predicted': 16, 'correct_edges': 8, 'fdr': 0.5, 'tpr': 0.1739, 'fpr': 0.006, 'shd': 41, 'nnz': 16}\n",
            "   G-DAG: {'total_edges_original': 46, 'total_edges_predicted': 1, 'correct_edges': 1, 'fdr': 0.0, 'tpr': 0.0217, 'fpr': 0.0, 'shd': 45, 'nnz': 1}\n",
            "Ep 0221/400  Reward=-189.774  eps=0.689\n",
            "Ep 0222/400  Reward=-222.682  eps=0.688\n",
            "Ep 0223/400  Reward=-230.382  eps=0.686\n",
            "Ep 0224/400  Reward=-221.493  eps=0.685\n",
            "Ep 0225/400  Reward=-149.357  eps=0.684\n",
            "Ep 0226/400  Reward=-216.767  eps=0.682\n",
            "Ep 0227/400  Reward=-224.040  eps=0.681\n",
            "Ep 0228/400  Reward=-154.853  eps=0.679\n",
            "Ep 0229/400  Reward=-223.580  eps=0.678\n",
            "Ep 0230/400  Reward=-193.825  eps=0.677\n",
            "Ep 0231/400  Reward=-183.000  eps=0.675\n",
            "Ep 0232/400  Reward=-222.738  eps=0.674\n",
            "Ep 0233/400  Reward=-142.676  eps=0.672\n",
            "Ep 0234/400  Reward=-181.785  eps=0.671\n",
            "Ep 0235/400  Reward=-259.393  eps=0.670\n",
            "Ep 0236/400  Reward=-180.558  eps=0.668\n",
            "Ep 0237/400  Reward=-151.488  eps=0.667\n",
            "Ep 0238/400  Reward=-179.911  eps=0.665\n",
            "Ep 0239/400  Reward=-150.866  eps=0.664\n",
            "Ep 0240/400  Reward=-168.541  eps=0.663 | ValBIC(A)=-192857.9  ValBIC(G)=-203003.2\n",
            "   A(raw): {'total_edges_original': 46, 'total_edges_predicted': 39, 'correct_edges': 8, 'fdr': 0.7949, 'tpr': 0.1739, 'fpr': 0.0234, 'shd': 66, 'nnz': 39}\n",
            "   A+CAM: {'total_edges_original': 46, 'total_edges_predicted': 16, 'correct_edges': 8, 'fdr': 0.5, 'tpr': 0.1739, 'fpr': 0.006, 'shd': 43, 'nnz': 16}\n",
            "   G-DAG: {'total_edges_original': 46, 'total_edges_predicted': 1, 'correct_edges': 1, 'fdr': 0.0, 'tpr': 0.0217, 'fpr': 0.0, 'shd': 45, 'nnz': 1}\n",
            "Ep 0241/400  Reward=-294.840  eps=0.661\n",
            "Ep 0242/400  Reward=-206.885  eps=0.660\n",
            "Ep 0243/400  Reward=-273.017  eps=0.658\n",
            "Ep 0244/400  Reward=-201.464  eps=0.657\n",
            "Ep 0245/400  Reward=-160.067  eps=0.656\n",
            "Ep 0246/400  Reward=-212.222  eps=0.654\n",
            "Ep 0247/400  Reward=-140.221  eps=0.653\n",
            "Ep 0248/400  Reward=-192.854  eps=0.651\n",
            "Ep 0249/400  Reward=-243.062  eps=0.650\n",
            "Ep 0250/400  Reward=-180.744  eps=0.649\n",
            "Ep 0251/400  Reward=-235.411  eps=0.647\n",
            "Ep 0252/400  Reward=-193.574  eps=0.646\n",
            "Ep 0253/400  Reward=-204.596  eps=0.644\n",
            "Ep 0254/400  Reward=-191.646  eps=0.643\n",
            "Ep 0255/400  Reward=-185.803  eps=0.641\n",
            "Ep 0256/400  Reward=-162.625  eps=0.640\n",
            "Ep 0257/400  Reward=-206.507  eps=0.639\n",
            "Ep 0258/400  Reward=-149.898  eps=0.637\n",
            "Ep 0259/400  Reward=-232.364  eps=0.636\n",
            "Ep 0260/400  Reward=-155.888  eps=0.634 | ValBIC(A)=-192310.9  ValBIC(G)=-203003.2\n",
            "   A(raw): {'total_edges_original': 46, 'total_edges_predicted': 40, 'correct_edges': 7, 'fdr': 0.825, 'tpr': 0.1522, 'fpr': 0.0249, 'shd': 67, 'nnz': 40}\n",
            "   A+CAM: {'total_edges_original': 46, 'total_edges_predicted': 16, 'correct_edges': 7, 'fdr': 0.5625, 'tpr': 0.1522, 'fpr': 0.0068, 'shd': 43, 'nnz': 16}\n",
            "   G-DAG: {'total_edges_original': 46, 'total_edges_predicted': 1, 'correct_edges': 1, 'fdr': 0.0, 'tpr': 0.0217, 'fpr': 0.0, 'shd': 45, 'nnz': 1}\n",
            "Ep 0261/400  Reward=-194.773  eps=0.633\n",
            "Ep 0262/400  Reward=-189.005  eps=0.632\n",
            "Ep 0263/400  Reward=-197.333  eps=0.630\n",
            "Ep 0264/400  Reward=-200.327  eps=0.629\n",
            "Ep 0265/400  Reward=-221.041  eps=0.627\n",
            "Ep 0266/400  Reward=-174.159  eps=0.626\n",
            "Ep 0267/400  Reward=-159.550  eps=0.625\n",
            "Ep 0268/400  Reward=-212.473  eps=0.623\n",
            "Ep 0269/400  Reward=-208.342  eps=0.622\n",
            "Ep 0270/400  Reward=-172.346  eps=0.620\n",
            "Ep 0271/400  Reward=-175.908  eps=0.619\n",
            "Ep 0272/400  Reward=-163.105  eps=0.618\n",
            "Ep 0273/400  Reward=-165.278  eps=0.616\n",
            "Ep 0274/400  Reward=-157.108  eps=0.615\n",
            "Ep 0275/400  Reward=-175.363  eps=0.613\n",
            "Ep 0276/400  Reward=-161.991  eps=0.612\n",
            "Ep 0277/400  Reward=-135.531  eps=0.611\n",
            "Ep 0278/400  Reward=-146.004  eps=0.609\n",
            "Ep 0279/400  Reward=-144.320  eps=0.608\n",
            "Ep 0280/400  Reward=-137.548  eps=0.606 | ValBIC(A)=-191967.4  ValBIC(G)=-203003.2\n",
            "   A(raw): {'total_edges_original': 46, 'total_edges_predicted': 40, 'correct_edges': 7, 'fdr': 0.825, 'tpr': 0.1522, 'fpr': 0.0249, 'shd': 65, 'nnz': 40}\n",
            "   A+CAM: {'total_edges_original': 46, 'total_edges_predicted': 16, 'correct_edges': 7, 'fdr': 0.5625, 'tpr': 0.1522, 'fpr': 0.0068, 'shd': 43, 'nnz': 16}\n",
            "   G-DAG: {'total_edges_original': 46, 'total_edges_predicted': 1, 'correct_edges': 1, 'fdr': 0.0, 'tpr': 0.0217, 'fpr': 0.0, 'shd': 45, 'nnz': 1}\n",
            "Ep 0281/400  Reward=-202.770  eps=0.605\n",
            "Ep 0282/400  Reward=-144.428  eps=0.604\n",
            "Ep 0283/400  Reward=-165.241  eps=0.602\n",
            "Ep 0284/400  Reward=-164.500  eps=0.601\n",
            "Ep 0285/400  Reward=-215.232  eps=0.599\n",
            "Ep 0286/400  Reward=-138.414  eps=0.598\n",
            "Ep 0287/400  Reward=-198.010  eps=0.596\n",
            "Ep 0288/400  Reward=-158.764  eps=0.595\n",
            "Ep 0289/400  Reward=-188.473  eps=0.594\n",
            "Ep 0290/400  Reward=-182.471  eps=0.592\n",
            "Ep 0291/400  Reward=-249.094  eps=0.591\n",
            "Ep 0292/400  Reward=-171.218  eps=0.589\n",
            "Ep 0293/400  Reward=-207.106  eps=0.588\n",
            "Ep 0294/400  Reward=-235.886  eps=0.587\n",
            "Ep 0295/400  Reward=-154.375  eps=0.585\n",
            "Ep 0296/400  Reward=-157.469  eps=0.584\n",
            "Ep 0297/400  Reward=-216.501  eps=0.582\n",
            "Ep 0298/400  Reward=-206.155  eps=0.581\n",
            "Ep 0299/400  Reward=-149.337  eps=0.580\n",
            "Ep 0300/400  Reward=-239.914  eps=0.578 | ValBIC(A)=-193336.3  ValBIC(G)=-203003.2\n",
            "   A(raw): {'total_edges_original': 46, 'total_edges_predicted': 40, 'correct_edges': 8, 'fdr': 0.8, 'tpr': 0.1739, 'fpr': 0.0242, 'shd': 67, 'nnz': 40}\n",
            "   A+CAM: {'total_edges_original': 46, 'total_edges_predicted': 14, 'correct_edges': 8, 'fdr': 0.4286, 'tpr': 0.1739, 'fpr': 0.0045, 'shd': 41, 'nnz': 14}\n",
            "   G-DAG: {'total_edges_original': 46, 'total_edges_predicted': 1, 'correct_edges': 1, 'fdr': 0.0, 'tpr': 0.0217, 'fpr': 0.0, 'shd': 45, 'nnz': 1}\n",
            "Ep 0301/400  Reward=-142.372  eps=0.577\n",
            "Ep 0302/400  Reward=-184.705  eps=0.575\n",
            "Ep 0303/400  Reward=-175.177  eps=0.574\n",
            "Ep 0304/400  Reward=-157.956  eps=0.573\n",
            "Ep 0305/400  Reward=-169.914  eps=0.571\n",
            "Ep 0306/400  Reward=-188.351  eps=0.570\n",
            "Ep 0307/400  Reward=-223.683  eps=0.568\n",
            "Ep 0308/400  Reward=-158.062  eps=0.567\n",
            "Ep 0309/400  Reward=-185.442  eps=0.566\n",
            "Ep 0310/400  Reward=-169.366  eps=0.564\n",
            "Ep 0311/400  Reward=-152.770  eps=0.563\n",
            "Ep 0312/400  Reward=-208.395  eps=0.561\n",
            "Ep 0313/400  Reward=-234.790  eps=0.560\n",
            "Ep 0314/400  Reward=-163.394  eps=0.559\n",
            "Ep 0315/400  Reward=-179.477  eps=0.557\n",
            "Ep 0316/400  Reward=-203.820  eps=0.556\n",
            "Ep 0317/400  Reward=-201.578  eps=0.554\n",
            "Ep 0318/400  Reward=-174.947  eps=0.553\n",
            "Ep 0319/400  Reward=-236.810  eps=0.551\n",
            "Ep 0320/400  Reward=-199.716  eps=0.550 | ValBIC(A)=-193917.0  ValBIC(G)=-203003.2\n",
            "   A(raw): {'total_edges_original': 46, 'total_edges_predicted': 40, 'correct_edges': 6, 'fdr': 0.85, 'tpr': 0.1304, 'fpr': 0.0257, 'shd': 67, 'nnz': 40}\n",
            "   A+CAM: {'total_edges_original': 46, 'total_edges_predicted': 15, 'correct_edges': 6, 'fdr': 0.6, 'tpr': 0.1304, 'fpr': 0.0068, 'shd': 43, 'nnz': 15}\n",
            "   G-DAG: {'total_edges_original': 46, 'total_edges_predicted': 1, 'correct_edges': 1, 'fdr': 0.0, 'tpr': 0.0217, 'fpr': 0.0, 'shd': 45, 'nnz': 1}\n",
            "Ep 0321/400  Reward=-160.493  eps=0.549\n",
            "Ep 0322/400  Reward=-179.581  eps=0.547\n",
            "Ep 0323/400  Reward=-198.022  eps=0.546\n",
            "Ep 0324/400  Reward=-194.091  eps=0.544\n",
            "Ep 0325/400  Reward=-218.619  eps=0.543\n",
            "Ep 0326/400  Reward=-128.806  eps=0.542\n",
            "Ep 0327/400  Reward=-148.315  eps=0.540\n",
            "Ep 0328/400  Reward=-195.465  eps=0.539\n",
            "Ep 0329/400  Reward=-191.733  eps=0.537\n",
            "Ep 0330/400  Reward=-179.281  eps=0.536\n",
            "Ep 0331/400  Reward=-218.428  eps=0.535\n",
            "Ep 0332/400  Reward=-156.858  eps=0.533\n",
            "Ep 0333/400  Reward=-194.099  eps=0.532\n",
            "Ep 0334/400  Reward=-157.207  eps=0.530\n",
            "Ep 0335/400  Reward=-226.496  eps=0.529\n",
            "Ep 0336/400  Reward=-149.983  eps=0.528\n",
            "Ep 0337/400  Reward=-180.301  eps=0.526\n",
            "Ep 0338/400  Reward=-143.598  eps=0.525\n",
            "Ep 0339/400  Reward=-198.959  eps=0.523\n",
            "Ep 0340/400  Reward=-200.103  eps=0.522 | ValBIC(A)=-193930.9  ValBIC(G)=-203003.2\n",
            "   A(raw): {'total_edges_original': 46, 'total_edges_predicted': 40, 'correct_edges': 6, 'fdr': 0.85, 'tpr': 0.1304, 'fpr': 0.0257, 'shd': 70, 'nnz': 40}\n",
            "   A+CAM: {'total_edges_original': 46, 'total_edges_predicted': 16, 'correct_edges': 6, 'fdr': 0.625, 'tpr': 0.1304, 'fpr': 0.0076, 'shd': 46, 'nnz': 16}\n",
            "   G-DAG: {'total_edges_original': 46, 'total_edges_predicted': 1, 'correct_edges': 1, 'fdr': 0.0, 'tpr': 0.0217, 'fpr': 0.0, 'shd': 45, 'nnz': 1}\n",
            "Ep 0341/400  Reward=-195.658  eps=0.521\n",
            "Ep 0342/400  Reward=-190.688  eps=0.519\n",
            "Ep 0343/400  Reward=-195.332  eps=0.518\n",
            "Ep 0344/400  Reward=-153.569  eps=0.516\n",
            "Ep 0345/400  Reward=-227.388  eps=0.515\n",
            "Ep 0346/400  Reward=-173.710  eps=0.514\n",
            "Ep 0347/400  Reward=-141.993  eps=0.512\n",
            "Ep 0348/400  Reward=-176.983  eps=0.511\n",
            "Ep 0349/400  Reward=-204.551  eps=0.509\n",
            "Ep 0350/400  Reward=-175.164  eps=0.508\n",
            "Ep 0351/400  Reward=-129.465  eps=0.506\n",
            "Ep 0352/400  Reward=-210.106  eps=0.505\n",
            "Ep 0353/400  Reward=-141.246  eps=0.504\n",
            "Ep 0354/400  Reward=-147.939  eps=0.502\n",
            "Ep 0355/400  Reward=-196.370  eps=0.501\n",
            "Ep 0356/400  Reward=-220.762  eps=0.499\n",
            "Ep 0357/400  Reward=-177.772  eps=0.498\n",
            "Ep 0358/400  Reward=-176.329  eps=0.497\n",
            "Ep 0359/400  Reward=-136.858  eps=0.495\n",
            "Ep 0360/400  Reward=-145.263  eps=0.494 | ValBIC(A)=-192423.7  ValBIC(G)=-203003.2\n",
            "   A(raw): {'total_edges_original': 46, 'total_edges_predicted': 40, 'correct_edges': 11, 'fdr': 0.725, 'tpr': 0.2391, 'fpr': 0.0219, 'shd': 60, 'nnz': 40}\n",
            "   A+CAM: {'total_edges_original': 46, 'total_edges_predicted': 15, 'correct_edges': 8, 'fdr': 0.4667, 'tpr': 0.1739, 'fpr': 0.0053, 'shd': 41, 'nnz': 15}\n",
            "   G-DAG: {'total_edges_original': 46, 'total_edges_predicted': 1, 'correct_edges': 1, 'fdr': 0.0, 'tpr': 0.0217, 'fpr': 0.0, 'shd': 45, 'nnz': 1}\n",
            "Ep 0361/400  Reward=-160.654  eps=0.492\n",
            "Ep 0362/400  Reward=-145.323  eps=0.491\n",
            "Ep 0363/400  Reward=-230.080  eps=0.490\n",
            "Ep 0364/400  Reward=-152.470  eps=0.488\n",
            "Ep 0365/400  Reward=-181.404  eps=0.487\n",
            "Ep 0366/400  Reward=-214.481  eps=0.485\n",
            "Ep 0367/400  Reward=-163.411  eps=0.484\n",
            "Ep 0368/400  Reward=-183.579  eps=0.483\n",
            "Ep 0369/400  Reward=-178.193  eps=0.481\n",
            "Ep 0370/400  Reward=-196.969  eps=0.480\n",
            "Ep 0371/400  Reward=-169.070  eps=0.478\n",
            "Ep 0372/400  Reward=-152.815  eps=0.477\n",
            "Ep 0373/400  Reward=-176.027  eps=0.476\n",
            "Ep 0374/400  Reward=-139.463  eps=0.474\n",
            "Ep 0375/400  Reward=-152.694  eps=0.473\n",
            "Ep 0376/400  Reward=-116.827  eps=0.471\n",
            "Ep 0377/400  Reward=-135.689  eps=0.470\n",
            "Ep 0378/400  Reward=-164.688  eps=0.469\n",
            "Ep 0379/400  Reward=-127.907  eps=0.467\n",
            "Ep 0380/400  Reward=-183.404  eps=0.466 | ValBIC(A)=-192160.1  ValBIC(G)=-203003.2\n",
            "   A(raw): {'total_edges_original': 46, 'total_edges_predicted': 40, 'correct_edges': 11, 'fdr': 0.725, 'tpr': 0.2391, 'fpr': 0.0219, 'shd': 61, 'nnz': 40}\n",
            "   A+CAM: {'total_edges_original': 46, 'total_edges_predicted': 17, 'correct_edges': 11, 'fdr': 0.3529, 'tpr': 0.2391, 'fpr': 0.0045, 'shd': 38, 'nnz': 17}\n",
            "   G-DAG: {'total_edges_original': 46, 'total_edges_predicted': 1, 'correct_edges': 1, 'fdr': 0.0, 'tpr': 0.0217, 'fpr': 0.0, 'shd': 45, 'nnz': 1}\n",
            "Ep 0381/400  Reward=-187.792  eps=0.464\n",
            "Ep 0382/400  Reward=-156.160  eps=0.463\n",
            "Ep 0383/400  Reward=-154.473  eps=0.462\n",
            "Ep 0384/400  Reward=-135.345  eps=0.460\n",
            "Ep 0385/400  Reward=-188.761  eps=0.459\n",
            "Ep 0386/400  Reward=-181.327  eps=0.457\n",
            "Ep 0387/400  Reward=-159.595  eps=0.456\n",
            "Ep 0388/400  Reward=-216.746  eps=0.454\n",
            "Ep 0389/400  Reward=-161.839  eps=0.453\n",
            "Ep 0390/400  Reward=-166.297  eps=0.452\n",
            "Ep 0391/400  Reward=-164.787  eps=0.450\n",
            "Ep 0392/400  Reward=-148.551  eps=0.449\n",
            "Ep 0393/400  Reward=-190.740  eps=0.447\n",
            "Ep 0394/400  Reward=-201.162  eps=0.446\n",
            "Ep 0395/400  Reward=-187.369  eps=0.445\n",
            "Ep 0396/400  Reward=-189.184  eps=0.443\n",
            "Ep 0397/400  Reward=-116.164  eps=0.442\n",
            "Ep 0398/400  Reward=-145.481  eps=0.440\n",
            "Ep 0399/400  Reward=-163.679  eps=0.439\n",
            "Ep 0400/400  Reward=-143.493  eps=0.438 | ValBIC(A)=-191845.4  ValBIC(G)=-203003.2\n",
            "   A(raw): {'total_edges_original': 46, 'total_edges_predicted': 40, 'correct_edges': 9, 'fdr': 0.775, 'tpr': 0.1957, 'fpr': 0.0234, 'shd': 64, 'nnz': 40}\n",
            "   A+CAM: {'total_edges_original': 46, 'total_edges_predicted': 17, 'correct_edges': 9, 'fdr': 0.4706, 'tpr': 0.1957, 'fpr': 0.006, 'shd': 41, 'nnz': 17}\n",
            "   G-DAG: {'total_edges_original': 46, 'total_edges_predicted': 1, 'correct_edges': 1, 'fdr': 0.0, 'tpr': 0.0217, 'fpr': 0.0, 'shd': 45, 'nnz': 1}\n",
            "\n",
            "Training finished.\n",
            "\n",
            "--- Final (Validation) ---\n",
            "ValBIC(agent):     -191845.43712228318\n",
            "ValBIC(agent+CAM): -191446.52316888171\n",
            "ValBIC(GraN-DAG):  -203003.22062962767\n",
            "\n",
            "--- Final (GT) Metrics ---\n",
            "Agent (raw):  {'total_edges_original': 46, 'total_edges_predicted': 40, 'correct_edges': 9, 'fdr': 0.775, 'tpr': 0.1957, 'fpr': 0.0234, 'shd': 64, 'nnz': 40}\n",
            "Agent+CAM:   {'total_edges_original': 46, 'total_edges_predicted': 17, 'correct_edges': 9, 'fdr': 0.4706, 'tpr': 0.1957, 'fpr': 0.006, 'shd': 41, 'nnz': 17}\n",
            "GraN-DAG:    {'total_edges_original': 46, 'total_edges_predicted': 1, 'correct_edges': 1, 'fdr': 0.0, 'tpr': 0.0217, 'fpr': 0.0, 'shd': 45, 'nnz': 1}\n",
            "\n",
            "--- Best-by-Validation snapshot ---\n",
            "A(best ValBIC):  {'total_edges_original': 46, 'total_edges_predicted': 40, 'correct_edges': 9, 'fdr': 0.775, 'tpr': 0.1957, 'fpr': 0.0234, 'shd': 64, 'nnz': 40}\n",
            "\n",
            "--- Best-by-TPR snapshot (analysis only) ---\n",
            "Best TPR = 0.2391\n",
            "A(best TPR):  {'total_edges_original': 46, 'total_edges_predicted': 40, 'correct_edges': 11, 'fdr': 0.725, 'tpr': 0.2391, 'fpr': 0.0219, 'shd': 60, 'nnz': 40}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section11"
      ],
      "metadata": {
        "id": "bnppyc-Iz7A1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dag_discovery_nogt.py\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Causal-Discovery (DQN vs. GraN-DAG) — NO GT VERSION\n",
        "\n",
        "- Opponent: GraN-DAG (iterations configurable)\n",
        "- Scoring: Gaussian BIC or Gaussian-Copula (nonparanormal) BIC on held-out validation\n",
        "- Warm-start: greedy add/prune/reverse by ValBIC with edge budget\n",
        "- Agent: Double DQN + Polyak target updates (no GT)\n",
        "- Reward: Δ ValBIC - λ1 * edges - action_cost - small step penalty (no GT)\n",
        "- Early stopping: by Validation BIC ONLY (no GT)\n",
        "- SAVES the adjacency with BEST ValBIC immediately to --out (and optional --ckpt)\n",
        "\n",
        "Run:\n",
        "  python dag_discovery_nogt.py --data /content/data.csv --out /content/agent_adj.npy \\\n",
        "         --iters 1000 --episodes 400 --eval-every 20 --score copula\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import warnings\n",
        "from collections import deque\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import networkx as nx\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from scipy.stats import rankdata, norm\n",
        "\n",
        "from castle.algorithms import GraNDAG\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# --------------------------- Utilities ---------------------------\n",
        "def set_seed(seed: int):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def load_data(csv_path: str):\n",
        "    if not os.path.exists(csv_path):\n",
        "        raise FileNotFoundError(f\"Data file not found: {csv_path}\")\n",
        "    df = pd.read_csv(csv_path, header=0)\n",
        "    if df.columns[0].lower().startswith(\"unnamed\"):\n",
        "        df = pd.read_csv(csv_path, header=0, index_col=0)\n",
        "    df = df.apply(pd.to_numeric, errors=\"coerce\").dropna(axis=1, how=\"all\")\n",
        "    X = df.values.astype(np.float64)\n",
        "    mu = X.mean(axis=0, keepdims=True)\n",
        "    sd = X.std(axis=0, keepdims=True); sd[sd == 0] = 1.0\n",
        "    X = (X - mu) / sd\n",
        "    print(f\"[data] Loaded: {X.shape[0]} samples, {X.shape[1]} vars\")\n",
        "    return X\n",
        "\n",
        "def binarize(A):\n",
        "    B = (np.asarray(A) != 0).astype(int)\n",
        "    np.fill_diagonal(B, 0)\n",
        "    return B\n",
        "\n",
        "\n",
        "def get_grandag_adj(data, iterations=1000, hidden_dim=16, hidden_num=2, lr=5e-4, h_threshold=1e-6):\n",
        "    print(f\"\\nRunning GraN-DAG (iterations={iterations})...\")\n",
        "    model = GraNDAG(\n",
        "        input_dim=data.shape[1], hidden_dim=hidden_dim, hidden_num=hidden_num,\n",
        "        lr=lr, iterations=iterations, h_threshold=h_threshold, mu_init=1e-3\n",
        "    )\n",
        "    model.learn(data)\n",
        "    print(\"GraN-DAG done.\")\n",
        "    return np.array(model.causal_matrix, dtype=np.float64)\n",
        "\n",
        "\n",
        "\n",
        "# # --------------------------- Opponent ---------------------------\n",
        "# def get_grandag_adj(data,\n",
        "#                     iterations=1000,\n",
        "#                     hidden_dim=16,\n",
        "#                     hidden_num=2,\n",
        "#                     lr=5e-4,\n",
        "#                     h_threshold=1e-6):\n",
        "#     device_type = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
        "#     print(f\"[GraN-DAG] iters={iterations}, device={device_type}\")\n",
        "#     model = GraNDAG(\n",
        "#         input_dim=data.shape[1],\n",
        "#         hidden_dim=hidden_dim,\n",
        "#         hidden_num=hidden_num,\n",
        "#         lr=lr,\n",
        "#         iterations=iterations,\n",
        "#         h_threshold=h_threshold,\n",
        "#         mu_init=1e-3,\n",
        "#         model_name=\"NonLinGauss\",\n",
        "#         nonlinear=\"leaky-relu\",\n",
        "#         optimizer=\"sgd\",\n",
        "#         norm_prod=\"paths\",\n",
        "#         device_type=device_type,\n",
        "#     )\n",
        "#     model.learn(data)\n",
        "#     G = np.array(model.causal_matrix, dtype=np.float64)\n",
        "#     print(\"[GraN-DAG] done.\")\n",
        "#     return G\n",
        "\n",
        "# --------------------------- Scorers ---------------------------\n",
        "class GaussianBIC:\n",
        "    def __init__(self, X: np.ndarray):\n",
        "        X = np.asarray(X, dtype=np.float64)\n",
        "        self.X = X\n",
        "        self.n, self.p = X.shape\n",
        "        self.S = X.T @ X\n",
        "        self.yTy = np.diag(self.S)\n",
        "\n",
        "    def node_rss(self, parents, j):\n",
        "        if len(parents) == 0:\n",
        "            y = self.X[:, j]\n",
        "            return float(((y - y.mean()) ** 2).sum())\n",
        "        P = np.array(parents, dtype=int)\n",
        "        Spp = self.S[np.ix_(P, P)]\n",
        "        Spy = self.S[P, j]\n",
        "        try:\n",
        "            coef = np.linalg.solve(Spp, Spy)\n",
        "        except np.linalg.LinAlgError:\n",
        "            coef = np.linalg.pinv(Spp) @ Spy\n",
        "        rss = self.yTy[j] - Spy @ coef\n",
        "        return float(max(rss, 1e-12))\n",
        "\n",
        "    def bic(self, A: np.ndarray):\n",
        "        n, p = self.n, self.p\n",
        "        A = (A != 0).astype(int)\n",
        "        total_rss = 0.0\n",
        "        num_params = p\n",
        "        for j in range(p):\n",
        "            parents = np.where(A[:, j] == 1)[0].tolist()\n",
        "            total_rss += self.node_rss(parents, j)\n",
        "            num_params += len(parents)\n",
        "        dof = max(n - num_params, 1)\n",
        "        sigma2 = max(total_rss / dof, 1e-9)\n",
        "        loglik = -0.5 * n * (p * np.log(2 * np.pi * sigma2) + 1.0)\n",
        "        return float(loglik - 0.5 * num_params * np.log(n))\n",
        "\n",
        "def gaussian_copula_transform(X):\n",
        "    X = np.asarray(X, dtype=np.float64)\n",
        "    n, p = X.shape\n",
        "    Z = np.empty_like(X)\n",
        "    eps = 1e-6\n",
        "    rng = np.random.default_rng(0)\n",
        "    for j in range(p):\n",
        "        x = X[:, j]\n",
        "        if np.std(x) < 1e-12:\n",
        "            x = x + rng.normal(0, 1e-9, size=n)\n",
        "        r = rankdata(x, method=\"average\")\n",
        "        u = (r - 0.5) / n\n",
        "        u = np.clip(u, eps, 1 - eps)\n",
        "        Z[:, j] = norm.ppf(u)\n",
        "    Z -= Z.mean(axis=0, keepdims=True)\n",
        "    std = Z.std(axis=0, keepdims=True); std[std == 0] = 1.0\n",
        "    Z /= std\n",
        "    return Z\n",
        "\n",
        "class CopulaBIC(GaussianBIC):\n",
        "    def __init__(self, X):\n",
        "        Z = gaussian_copula_transform(X)\n",
        "        super().__init__(Z)\n",
        "\n",
        "# --------------------------- Warm-start ---------------------------\n",
        "def warm_start_greedy_bic(Xtr, Xva, edge_budget, scorer_cls=GaussianBIC,\n",
        "                          max_passes=5, topk_per_pass=5, restarts=5, seed=42):\n",
        "    p = Xtr.shape[1]\n",
        "    fb = scorer_cls(Xva)\n",
        "\n",
        "    def is_dag(M): return nx.is_directed_acyclic_graph(nx.DiGraph(M))\n",
        "\n",
        "    def one_run():\n",
        "        A = np.zeros((p, p), dtype=np.float64)\n",
        "        best = fb.bic(A)\n",
        "        for _ in range(max_passes):\n",
        "            improved = False\n",
        "            # forward (top-k adds)\n",
        "            cands = []\n",
        "            if A.sum() < edge_budget:\n",
        "                for i in range(p):\n",
        "                    for j in range(p):\n",
        "                        if i == j or A[i, j] == 1: continue\n",
        "                        if A.sum() >= edge_budget: break\n",
        "                        T = A.copy(); T[i, j] = 1.0; np.fill_diagonal(T, 0.0)\n",
        "                        if not is_dag(T): continue\n",
        "                        s = fb.bic(T)\n",
        "                        if s > best: cands.append((s - best, i, j))\n",
        "                cands.sort(reverse=True, key=lambda x: x[0])\n",
        "                for _, i, j in cands[:topk_per_pass]:\n",
        "                    if A.sum() >= edge_budget: break\n",
        "                    T = A.copy(); T[i, j] = 1.0; np.fill_diagonal(T, 0.0)\n",
        "                    if not is_dag(T): continue\n",
        "                    s = fb.bic(T)\n",
        "                    if s > best:\n",
        "                        A, best, improved = T, s, True\n",
        "            # backward (prune)\n",
        "            pruned = True\n",
        "            while pruned:\n",
        "                pruned = False\n",
        "                for i in range(p):\n",
        "                    for j in range(p):\n",
        "                        if A[i, j] == 0: continue\n",
        "                        T = A.copy(); T[i, j] = 0.0\n",
        "                        s = fb.bic(T)\n",
        "                        if s > best:\n",
        "                            A, best, improved, pruned = T, s, True, True\n",
        "            # reversals\n",
        "            for i in range(p):\n",
        "                for j in range(p):\n",
        "                    if A[i, j] != 1: continue\n",
        "                    T = A.copy(); T[i, j] = 0.0; T[j, i] = 1.0\n",
        "                    if not is_dag(T): continue\n",
        "                    s = fb.bic(T)\n",
        "                    if s > best:\n",
        "                        A, best, improved = T, s, True\n",
        "            if not improved: break\n",
        "        return A, best\n",
        "\n",
        "    bestA, bestS = None, -np.inf\n",
        "    for _ in range(restarts):\n",
        "        A, s = one_run()\n",
        "        if s > bestS:\n",
        "            bestA, bestS = A, s\n",
        "    return (bestA != 0).astype(int)\n",
        "\n",
        "# --------------------------- Environment ---------------------------\n",
        "class CausalDiscoveryEnv:\n",
        "    \"\"\"\n",
        "    Reward = Δ Val-BIC - λ1 * edges - action_cost - small step penalty.\n",
        "    \"\"\"\n",
        "    def __init__(self, data, grandag_adj,\n",
        "                 val_frac=0.2,\n",
        "                 edge_budget_ratio=1.1,\n",
        "                 lambda_l1=0.02, action_cost=0.05,\n",
        "                 warm_start=True,\n",
        "                 score_type=\"copula\",\n",
        "                 seed=42):\n",
        "        self.full = data\n",
        "        self.n_samples, self.n_nodes = data.shape\n",
        "        self.grandag_adj = (grandag_adj != 0).astype(int)\n",
        "\n",
        "        idx = np.arange(self.n_samples); rng = np.random.RandomState(seed)\n",
        "        rng.shuffle(idx)\n",
        "        cut = int((1.0 - val_frac) * self.n_samples)\n",
        "        self.Xtr, self.Xva = self.full[idx[:cut]], self.full[idx[cut:]]\n",
        "\n",
        "        Scorer = CopulaBIC if str(score_type).lower() == \"copula\" else GaussianBIC\n",
        "        self.bic_va = Scorer(self.Xva)\n",
        "\n",
        "        self.state_space_shape = (self.n_nodes * self.n_nodes,)\n",
        "        self.n_actions = 3 * self.n_nodes * (self.n_nodes - 1)\n",
        "        self.action_map = self._create_action_map()\n",
        "\n",
        "        self.current_adj = np.zeros((self.n_nodes, self.n_nodes), dtype=np.float64)\n",
        "        self.max_steps = 10 * self.n_nodes\n",
        "        self.current_step = 0\n",
        "\n",
        "        self.edge_budget = int(max(1, edge_budget_ratio * self.n_nodes))\n",
        "        self.lambda_l1 = float(lambda_l1)\n",
        "        self.action_cost = float(action_cost)\n",
        "\n",
        "        self._warm_adj = None\n",
        "        if warm_start:\n",
        "            print(\"[warm] searching greedy init ...\")\n",
        "            self._warm_adj = warm_start_greedy_bic(\n",
        "                self.Xtr, self.Xva,\n",
        "                edge_budget=self.edge_budget,\n",
        "                scorer_cls=Scorer,\n",
        "                max_passes=5, topk_per_pass=5, restarts=5, seed=seed\n",
        "            )\n",
        "            print(f\"[warm] edges={int(np.sum(self._warm_adj))}\")\n",
        "\n",
        "    def _create_action_map(self):\n",
        "        mapping, idx = {}, 0\n",
        "        for i in range(self.n_nodes):\n",
        "            for j in range(self.n_nodes):\n",
        "                if i == j: continue\n",
        "                mapping[idx] = (\"add\", i, j); idx += 1\n",
        "                mapping[idx] = (\"remove\", i, j); idx += 1\n",
        "                mapping[idx] = (\"reverse\", i, j); idx += 1\n",
        "        return mapping\n",
        "\n",
        "    def _val_bic(self, A): return self.bic_va.bic(A)\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_step = 0\n",
        "        if self._warm_adj is not None:\n",
        "            self.current_adj = self._warm_adj.copy().astype(np.float64)\n",
        "        else:\n",
        "            self.current_adj[:] = 0.0\n",
        "        return self.current_adj.flatten().copy()\n",
        "\n",
        "    def step(self, action_idx: int):\n",
        "        op, i, j = self.action_map[action_idx]\n",
        "        prev_adj = self.current_adj.copy()\n",
        "\n",
        "        if op == \"add\" and np.sum(self.current_adj) >= self.edge_budget:\n",
        "            self.current_step += 1\n",
        "            return self.current_adj.flatten(), -0.2, self.current_step >= self.max_steps, {}\n",
        "\n",
        "        trial = self.current_adj.copy()\n",
        "        if op == \"add\":\n",
        "            trial[i, j] = 1.0\n",
        "        elif op == \"remove\":\n",
        "            trial[i, j] = 0.0\n",
        "        elif op == \"reverse\":\n",
        "            if self.current_adj[i, j] == 1.0:\n",
        "                trial[i, j] = 0.0; trial[j, i] = 1.0\n",
        "            else:\n",
        "                self.current_step += 1\n",
        "                return self.current_adj.flatten(), -0.2, self.current_step >= self.max_steps, {}\n",
        "        np.fill_diagonal(trial, 0.0)\n",
        "\n",
        "        if not nx.is_directed_acyclic_graph(nx.DiGraph(trial)):\n",
        "            self.current_step += 1\n",
        "            return self.current_adj.flatten(), -0.5, self.current_step >= self.max_steps, {}\n",
        "\n",
        "        self.current_adj = trial\n",
        "        self.current_step += 1\n",
        "        r = self._reward(prev_adj, self.current_adj)\n",
        "        done = self.current_step >= self.max_steps\n",
        "        return self.current_adj.flatten().copy(), r, done, {}\n",
        "\n",
        "    def _reward(self, prev_adj, new_adj):\n",
        "        prev = self._val_bic(prev_adj)\n",
        "        new  = self._val_bic(new_adj)\n",
        "        score = (new - prev) / max(self.n_nodes, 1)\n",
        "        score = float(np.clip(score, -100.0, 10.0))\n",
        "        step_pen = -0.002\n",
        "        sparsity = - self.lambda_l1 * float(np.sum(new_adj))\n",
        "        act_pen = - self.action_cost if (new_adj != prev_adj).any() else 0.0\n",
        "        total = score + sparsity + act_pen + step_pen\n",
        "        return float(np.clip(total, -100.0, 20.0))\n",
        "\n",
        "# --------------------------- Agent (Double DQN) ---------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DTYPE  = torch.double\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(state_size, 256, dtype=DTYPE),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 256, dtype=DTYPE),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, action_size, dtype=DTYPE),\n",
        "        )\n",
        "    def forward(self, x): return self.layers(x)\n",
        "\n",
        "class CausalAgent:\n",
        "    def __init__(self, state_size, action_size, lr=5e-4, gamma=0.95,\n",
        "                 eps_start=1.0, eps_end=0.05, eps_decay_steps=250_000,\n",
        "                 batch=256, buffer=120_000, tau=0.005):\n",
        "        self.action_size = action_size\n",
        "        self.q = QNetwork(state_size, action_size).to(device)\n",
        "        self.t = QNetwork(state_size, action_size).to(device)\n",
        "        self.t.load_state_dict(self.q.state_dict())\n",
        "        self.tau = tau\n",
        "        self.opt = optim.Adam(self.q.parameters(), lr=lr)\n",
        "        self.gamma = gamma\n",
        "\n",
        "        self.eps_start, self.eps_end = eps_start, eps_end\n",
        "        self.eps_decay_steps = eps_decay_steps\n",
        "        self.total_steps = 0\n",
        "        self.epsilon = eps_start\n",
        "\n",
        "        self.mem = deque(maxlen=buffer)\n",
        "        self.batch = batch\n",
        "\n",
        "    def _update_eps(self):\n",
        "        self.total_steps += 1\n",
        "        frac = min(1.0, self.total_steps / self.eps_decay_steps)\n",
        "        self.epsilon = self.eps_start + frac * (self.eps_end - self.eps_start)\n",
        "\n",
        "    def remember(self, s, a, r, ns, d): self.mem.append((s, a, r, ns, d))\n",
        "\n",
        "    def act(self, state):\n",
        "        self._update_eps()\n",
        "        if random.random() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        st = torch.tensor(state, dtype=DTYPE, device=device).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            qv = self.q(st)\n",
        "        return int(qv.argmax(dim=1).item())\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.mem) < self.batch: return\n",
        "        batch = random.sample(self.mem, self.batch)\n",
        "        s  = torch.tensor(np.array([e[0] for e in batch]), dtype=DTYPE, device=device)\n",
        "        a  = torch.tensor([e[1] for e in batch], dtype=torch.long, device=device).unsqueeze(1)\n",
        "        r  = torch.tensor([e[2] for e in batch], dtype=DTYPE, device=device).unsqueeze(1)\n",
        "        ns = torch.tensor(np.array([e[3] for e in batch]), dtype=DTYPE, device=device)\n",
        "        d  = torch.tensor([e[4] for e in batch], dtype=DTYPE, device=device).unsqueeze(1)\n",
        "\n",
        "        q_sa = self.q(s).gather(1, a)\n",
        "        with torch.no_grad():\n",
        "            na_online = self.q(ns).argmax(1, keepdim=True)\n",
        "            q_next = self.t(ns).gather(1, na_online)\n",
        "            target = r + (1.0 - d) * self.gamma * q_next\n",
        "\n",
        "        loss = nn.MSELoss()(q_sa, target)\n",
        "        self.opt.zero_grad(); loss.backward()\n",
        "        nn.utils.clip_grad_norm_(self.q.parameters(), 5.0)\n",
        "        self.opt.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for tparam, qparam in zip(self.t.parameters(), self.q.parameters()):\n",
        "                tparam.data.mul_(1 - self.tau).add_(self.tau * qparam.data)\n",
        "\n",
        "# --------------------------- CAM pruning (optional, not used for saving) ---------------------------\n",
        "def cam_prune_linear_from_A(A_directed, X, th=0.3):\n",
        "    A = (A_directed != 0).astype(int)\n",
        "    parents = A.T\n",
        "    d = parents.shape[0]\n",
        "    reg = LinearRegression()\n",
        "    W = []\n",
        "    for child in range(d):\n",
        "        col = parents[child] > 0\n",
        "        if np.sum(col) == 0:\n",
        "            W.append(np.zeros(d)); continue\n",
        "        Xp = X[:, col]\n",
        "        y  = X[:, child]\n",
        "        reg.fit(Xp, y)\n",
        "        coeff = reg.coef_\n",
        "        newc = np.zeros(d)\n",
        "        k = 0\n",
        "        for i in range(d):\n",
        "            if col[i]:\n",
        "                newc[i] = coeff[k]; k += 1\n",
        "        W.append(newc)\n",
        "    pruned_parents = (np.abs(np.vstack(W)) > th).astype(int)\n",
        "    return pruned_parents.T\n",
        "\n",
        "# --------------------------- Main ---------------------------\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--data\", required=True)\n",
        "    ap.add_argument(\"--out\", required=True)\n",
        "    ap.add_argument(\"--seed\", type=int, default=42)\n",
        "    ap.add_argument(\"--iters\", type=int, default=1000)\n",
        "    ap.add_argument(\"--episodes\", type=int, default=400)\n",
        "    ap.add_argument(\"--eval-every\", type=int, default=20)\n",
        "    ap.add_argument(\"--score\", choices=[\"gaussian\", \"copula\"], default=\"copula\")\n",
        "    ap.add_argument(\"--edge-budget-ratio\", type=float, default=1.1)\n",
        "    ap.add_argument(\"--lambda-l1\", type=float, default=0.02)\n",
        "    ap.add_argument(\"--action-cost\", type=float, default=0.05)\n",
        "    ap.add_argument(\"--patience\", type=int, default=10)\n",
        "    ap.add_argument(\"--min-delta\", type=float, default=1e-3)\n",
        "    ap.add_argument(\"--ckpt\", default=None, help=\"Optional extra path to also save best DAG\")\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    set_seed(args.seed)\n",
        "\n",
        "    X = load_data(args.data)\n",
        "    p = X.shape[1]\n",
        "\n",
        "    # Opponent (not used for saving, just as baseline for reporting)\n",
        "    Gdag = get_grandag_adj(X, iterations=args.iters)\n",
        "    if Gdag.shape != (p, p):\n",
        "        Gdag = Gdag[:p, :p]\n",
        "    Gdag_bin = binarize(Gdag)\n",
        "\n",
        "    # Env + Agent\n",
        "    env = CausalDiscoveryEnv(\n",
        "        X, Gdag,\n",
        "        val_frac=0.2,\n",
        "        edge_budget_ratio=args.edge_budget_ratio,\n",
        "        lambda_l1=args.lambda_l1,\n",
        "        action_cost=args.action_cost,\n",
        "        warm_start=True,\n",
        "        score_type=args.score,\n",
        "        seed=args.seed\n",
        "    )\n",
        "    agent = CausalAgent(state_size=env.state_space_shape[0], action_size=env.n_actions)\n",
        "\n",
        "    # Best-tracking (ValBIC only, NO GT); save immediately when improved\n",
        "    best_valbic = -np.inf\n",
        "    patience_ctr = 0\n",
        "\n",
        "    print(\"\\n[train] starting ...\")\n",
        "    for ep in range(args.episodes):\n",
        "        s = env.reset()\n",
        "        total = 0.0; done = False\n",
        "        while not done:\n",
        "            a = agent.act(s)\n",
        "            ns, r, done, _ = env.step(a)\n",
        "            agent.remember(s, a, r, ns, done)\n",
        "            s = ns; total += r\n",
        "            agent.replay()\n",
        "\n",
        "        print(f\"Ep {ep+1:04d}/{args.episodes} | Reward={total:9.3f} | eps={agent.epsilon:.3f}\")\n",
        "\n",
        "        if (ep + 1) % args.eval_every == 0:\n",
        "            A_now = binarize(env.current_adj)\n",
        "            vb_agent = env._val_bic(A_now)\n",
        "            vb_cam   = env._val_bic(cam_prune_linear_from_A(A_now, X, th=0.3))\n",
        "            vb_gd    = env._val_bic(Gdag_bin)\n",
        "\n",
        "            print(f\"  ValBIC | agent={vb_agent: .3f} | agent+CAM={vb_cam: .3f} | GraN-DAG={vb_gd: .3f}\")\n",
        "\n",
        "            if vb_agent > best_valbic + args.min_delta:\n",
        "                best_valbic = vb_agent\n",
        "                np.save(args.out, A_now)  # <-- SAVE THE BEST ADJ IMMEDIATELY\n",
        "                if args.ckpt:\n",
        "                    np.save(args.ckpt, A_now)\n",
        "                patience_ctr = 0\n",
        "                print(f\"  [SAVE] Best ValBIC improved to {vb_agent:.6f} → {args.out}\")\n",
        "            else:\n",
        "                patience_ctr += 1\n",
        "                print(f\"  [ES] patience {patience_ctr}/{args.patience}\")\n",
        "\n",
        "            if patience_ctr >= args.patience:\n",
        "                print(\"  [ES] Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "    print(\"\\n[train] finished.\")\n",
        "\n",
        "    # If nothing improved (edge case), save the latest adjacency\n",
        "    if not os.path.exists(args.out):\n",
        "        A_final = binarize(env.current_adj)\n",
        "        np.save(args.out, A_final)\n",
        "        print(f\"[fallback] Saved latest adjacency → {args.out}\")\n",
        "\n",
        "    print(f\"[done] Best-by-ValBIC adjacency saved at: {args.out}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jloDsbwYkVVI",
        "outputId": "6aa9c5c1-ff2b-48a4-90c3-31b959dd8e48"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting dag_discovery_nogt.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python dag_discovery_nogt.py \\\n",
        "  --data /content/data.csv \\\n",
        "  --out  /content/agent_adj.npy \\\n",
        "  --iters 1000 \\\n",
        "  --episodes 380 \\\n",
        "  --eval-every 20 \\\n",
        "  --score copula \\\n",
        "  --patience 10 \\\n",
        "  --min-delta 1e-3\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XM0fYAZZkdy4",
        "outputId": "df269a83-7ef5-459a-983b-2a37809a9e69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-17 22:11:22,657 - /usr/local/lib/python3.11/dist-packages/castle/backend/__init__.py[line:36] - INFO: You can use `os.environ['CASTLE_BACKEND'] = backend` to set the backend(`pytorch` or `mindspore`).\n",
            "2025-08-17 22:11:22,715 - /usr/local/lib/python3.11/dist-packages/castle/algorithms/__init__.py[line:36] - INFO: You are using ``pytorch`` as the backend.\n",
            "[data] Loaded: 10000 samples, 37 vars\n",
            "\n",
            "Running GraN-DAG (iterations=1000)...\n",
            "2025-08-17 22:11:22,786 - /usr/local/lib/python3.11/dist-packages/castle/algorithms/gradient/gran_dag/torch/gran_dag.py[line:269] - INFO: GPU is available.\n",
            "Training Iterations: 100% 1000/1000 [00:26<00:00, 38.40it/s]\n",
            "GraN-DAG done.\n",
            "[warm] searching greedy init ...\n",
            "[warm] edges=15\n",
            "\n",
            "[train] starting ...\n",
            "Ep 0001/380 | Reward= -218.937 | eps=0.999\n",
            "Ep 0002/380 | Reward= -162.700 | eps=0.997\n",
            "Ep 0003/380 | Reward= -198.243 | eps=0.996\n",
            "Ep 0004/380 | Reward= -177.633 | eps=0.994\n",
            "Ep 0005/380 | Reward= -240.795 | eps=0.993\n",
            "Ep 0006/380 | Reward= -201.221 | eps=0.992\n",
            "Ep 0007/380 | Reward= -203.703 | eps=0.990\n",
            "Ep 0008/380 | Reward= -175.785 | eps=0.989\n",
            "Ep 0009/380 | Reward= -207.475 | eps=0.987\n",
            "Ep 0010/380 | Reward= -157.953 | eps=0.986\n",
            "Ep 0011/380 | Reward= -139.005 | eps=0.985\n",
            "Ep 0012/380 | Reward= -194.031 | eps=0.983\n",
            "Ep 0013/380 | Reward= -139.522 | eps=0.982\n",
            "Ep 0014/380 | Reward= -204.042 | eps=0.980\n",
            "Ep 0015/380 | Reward= -205.354 | eps=0.979\n",
            "Ep 0016/380 | Reward= -214.088 | eps=0.978\n",
            "Ep 0017/380 | Reward= -165.958 | eps=0.976\n",
            "Ep 0018/380 | Reward= -230.213 | eps=0.975\n",
            "Ep 0019/380 | Reward= -224.088 | eps=0.973\n",
            "Ep 0020/380 | Reward= -199.901 | eps=0.972\n",
            "  ValBIC | agent=-193334.569 | agent+CAM=-192770.794 | GraN-DAG=-203003.221\n",
            "  [SAVE] Best ValBIC improved to -193334.569014 → /content/agent_adj.npy\n",
            "Ep 0021/380 | Reward= -188.767 | eps=0.970\n",
            "Ep 0022/380 | Reward= -233.687 | eps=0.969\n",
            "Ep 0023/380 | Reward= -184.665 | eps=0.968\n",
            "Ep 0024/380 | Reward= -168.046 | eps=0.966\n",
            "Ep 0025/380 | Reward= -208.196 | eps=0.965\n",
            "Ep 0026/380 | Reward= -159.316 | eps=0.963\n",
            "Ep 0027/380 | Reward= -153.887 | eps=0.962\n",
            "Ep 0028/380 | Reward= -146.683 | eps=0.961\n",
            "Ep 0029/380 | Reward= -169.649 | eps=0.959\n",
            "Ep 0030/380 | Reward= -264.843 | eps=0.958\n",
            "Ep 0031/380 | Reward= -202.174 | eps=0.956\n",
            "Ep 0032/380 | Reward= -184.042 | eps=0.955\n",
            "Ep 0033/380 | Reward= -188.400 | eps=0.954\n",
            "Ep 0034/380 | Reward= -233.415 | eps=0.952\n",
            "Ep 0035/380 | Reward= -171.622 | eps=0.951\n",
            "Ep 0036/380 | Reward= -192.465 | eps=0.949\n",
            "Ep 0037/380 | Reward= -187.047 | eps=0.948\n",
            "Ep 0038/380 | Reward= -165.906 | eps=0.947\n",
            "Ep 0039/380 | Reward= -199.667 | eps=0.945\n",
            "Ep 0040/380 | Reward= -169.424 | eps=0.944\n",
            "  ValBIC | agent=-192537.162 | agent+CAM=-192009.845 | GraN-DAG=-203003.221\n",
            "  [SAVE] Best ValBIC improved to -192537.161788 → /content/agent_adj.npy\n",
            "Ep 0041/380 | Reward= -153.800 | eps=0.942\n",
            "Ep 0042/380 | Reward= -198.035 | eps=0.941\n",
            "Ep 0043/380 | Reward= -175.057 | eps=0.940\n",
            "Ep 0044/380 | Reward= -229.216 | eps=0.938\n",
            "Ep 0045/380 | Reward= -201.128 | eps=0.937\n",
            "Ep 0046/380 | Reward= -177.236 | eps=0.935\n",
            "Ep 0047/380 | Reward= -222.807 | eps=0.934\n",
            "Ep 0048/380 | Reward= -234.487 | eps=0.933\n",
            "Ep 0049/380 | Reward= -213.871 | eps=0.931\n",
            "Ep 0050/380 | Reward= -168.135 | eps=0.930\n",
            "Ep 0051/380 | Reward= -161.782 | eps=0.928\n",
            "Ep 0052/380 | Reward= -207.044 | eps=0.927\n",
            "Ep 0053/380 | Reward= -196.615 | eps=0.925\n",
            "Ep 0054/380 | Reward= -229.375 | eps=0.924\n",
            "Ep 0055/380 | Reward= -177.497 | eps=0.923\n",
            "Ep 0056/380 | Reward= -220.209 | eps=0.921\n",
            "Ep 0057/380 | Reward= -152.198 | eps=0.920\n",
            "Ep 0058/380 | Reward= -177.022 | eps=0.918\n",
            "Ep 0059/380 | Reward= -197.504 | eps=0.917\n",
            "Ep 0060/380 | Reward= -191.533 | eps=0.916\n",
            "  ValBIC | agent=-193299.405 | agent+CAM=-192801.105 | GraN-DAG=-203003.221\n",
            "  [ES] patience 1/10\n",
            "Ep 0061/380 | Reward= -196.074 | eps=0.914\n",
            "Ep 0062/380 | Reward= -203.459 | eps=0.913\n",
            "Ep 0063/380 | Reward= -176.961 | eps=0.911\n",
            "Ep 0064/380 | Reward= -193.358 | eps=0.910\n",
            "Ep 0065/380 | Reward= -167.789 | eps=0.909\n",
            "Ep 0066/380 | Reward= -170.026 | eps=0.907\n",
            "Ep 0067/380 | Reward= -244.585 | eps=0.906\n",
            "Ep 0068/380 | Reward= -158.461 | eps=0.904\n",
            "Ep 0069/380 | Reward= -207.518 | eps=0.903\n",
            "Ep 0070/380 | Reward= -192.257 | eps=0.902\n",
            "Ep 0071/380 | Reward= -211.458 | eps=0.900\n",
            "Ep 0072/380 | Reward= -218.971 | eps=0.899\n",
            "Ep 0073/380 | Reward= -193.735 | eps=0.897\n",
            "Ep 0074/380 | Reward= -200.219 | eps=0.896\n",
            "Ep 0075/380 | Reward= -144.343 | eps=0.895\n",
            "Ep 0076/380 | Reward= -175.166 | eps=0.893\n",
            "Ep 0077/380 | Reward= -196.600 | eps=0.892\n",
            "Ep 0078/380 | Reward= -243.673 | eps=0.890\n",
            "Ep 0079/380 | Reward= -193.295 | eps=0.889\n",
            "Ep 0080/380 | Reward= -210.511 | eps=0.888\n",
            "  ValBIC | agent=-194269.841 | agent+CAM=-193768.751 | GraN-DAG=-203003.221\n",
            "  [ES] patience 2/10\n",
            "Ep 0081/380 | Reward= -222.993 | eps=0.886\n",
            "Ep 0082/380 | Reward= -197.223 | eps=0.885\n",
            "Ep 0083/380 | Reward= -179.800 | eps=0.883\n",
            "Ep 0084/380 | Reward= -236.416 | eps=0.882\n",
            "Ep 0085/380 | Reward= -193.684 | eps=0.880\n",
            "Ep 0086/380 | Reward= -215.890 | eps=0.879\n",
            "Ep 0087/380 | Reward= -161.321 | eps=0.878\n",
            "Ep 0088/380 | Reward= -197.703 | eps=0.876\n",
            "Ep 0089/380 | Reward= -208.326 | eps=0.875\n",
            "Ep 0090/380 | Reward= -149.807 | eps=0.873\n",
            "Ep 0091/380 | Reward= -260.766 | eps=0.872\n",
            "Ep 0092/380 | Reward= -186.017 | eps=0.871\n",
            "Ep 0093/380 | Reward= -174.349 | eps=0.869\n",
            "Ep 0094/380 | Reward= -178.902 | eps=0.868\n",
            "Ep 0095/380 | Reward= -201.424 | eps=0.866\n",
            "Ep 0096/380 | Reward= -205.277 | eps=0.865\n",
            "Ep 0097/380 | Reward= -195.377 | eps=0.864\n",
            "Ep 0098/380 | Reward= -189.177 | eps=0.862\n",
            "Ep 0099/380 | Reward= -204.003 | eps=0.861\n",
            "Ep 0100/380 | Reward= -172.466 | eps=0.859\n",
            "  ValBIC | agent=-192975.712 | agent+CAM=-192549.056 | GraN-DAG=-203003.221\n",
            "  [ES] patience 3/10\n",
            "Ep 0101/380 | Reward= -180.618 | eps=0.858\n",
            "Ep 0102/380 | Reward= -204.559 | eps=0.857\n",
            "Ep 0103/380 | Reward= -161.186 | eps=0.855\n",
            "Ep 0104/380 | Reward= -228.595 | eps=0.854\n",
            "Ep 0105/380 | Reward= -213.916 | eps=0.852\n",
            "Ep 0106/380 | Reward= -264.003 | eps=0.851\n",
            "Ep 0107/380 | Reward= -202.197 | eps=0.850\n",
            "Ep 0108/380 | Reward= -215.777 | eps=0.848\n",
            "Ep 0109/380 | Reward= -232.298 | eps=0.847\n",
            "Ep 0110/380 | Reward= -227.390 | eps=0.845\n",
            "Ep 0111/380 | Reward= -206.881 | eps=0.844\n",
            "Ep 0112/380 | Reward= -207.780 | eps=0.843\n",
            "Ep 0113/380 | Reward= -218.358 | eps=0.841\n",
            "Ep 0114/380 | Reward= -189.390 | eps=0.840\n",
            "Ep 0115/380 | Reward= -166.364 | eps=0.838\n",
            "Ep 0116/380 | Reward= -243.250 | eps=0.837\n",
            "Ep 0117/380 | Reward= -192.403 | eps=0.835\n",
            "Ep 0118/380 | Reward= -175.054 | eps=0.834\n",
            "Ep 0119/380 | Reward= -169.878 | eps=0.833\n",
            "Ep 0120/380 | Reward= -193.403 | eps=0.831\n",
            "  ValBIC | agent=-192565.596 | agent+CAM=-192115.406 | GraN-DAG=-203003.221\n",
            "  [ES] patience 4/10\n",
            "Ep 0121/380 | Reward= -194.117 | eps=0.830\n",
            "Ep 0122/380 | Reward= -150.572 | eps=0.828\n",
            "Ep 0123/380 | Reward= -167.737 | eps=0.827\n",
            "Ep 0124/380 | Reward= -153.564 | eps=0.826\n",
            "Ep 0125/380 | Reward= -179.806 | eps=0.824\n",
            "Ep 0126/380 | Reward= -147.567 | eps=0.823\n",
            "Ep 0127/380 | Reward= -219.502 | eps=0.821\n",
            "Ep 0128/380 | Reward= -177.726 | eps=0.820\n",
            "Ep 0129/380 | Reward= -201.824 | eps=0.819\n",
            "Ep 0130/380 | Reward= -215.158 | eps=0.817\n",
            "Ep 0131/380 | Reward= -178.861 | eps=0.816\n",
            "Ep 0132/380 | Reward= -175.723 | eps=0.814\n",
            "Ep 0133/380 | Reward= -139.232 | eps=0.813\n",
            "Ep 0134/380 | Reward= -160.625 | eps=0.812\n",
            "Ep 0135/380 | Reward= -162.777 | eps=0.810\n",
            "Ep 0136/380 | Reward= -179.338 | eps=0.809\n",
            "Ep 0137/380 | Reward= -183.016 | eps=0.807\n",
            "Ep 0138/380 | Reward= -217.624 | eps=0.806\n",
            "Ep 0139/380 | Reward= -218.462 | eps=0.805\n",
            "Ep 0140/380 | Reward= -188.413 | eps=0.803\n",
            "  ValBIC | agent=-192824.827 | agent+CAM=-192315.508 | GraN-DAG=-203003.221\n",
            "  [ES] patience 5/10\n",
            "Ep 0141/380 | Reward= -216.537 | eps=0.802\n",
            "Ep 0142/380 | Reward= -182.102 | eps=0.800\n",
            "Ep 0143/380 | Reward= -173.934 | eps=0.799\n",
            "Ep 0144/380 | Reward= -177.431 | eps=0.798\n",
            "Ep 0145/380 | Reward= -229.737 | eps=0.796\n",
            "Ep 0146/380 | Reward= -237.598 | eps=0.795\n",
            "Ep 0147/380 | Reward= -190.678 | eps=0.793\n",
            "Ep 0148/380 | Reward= -171.039 | eps=0.792\n",
            "Ep 0149/380 | Reward= -165.612 | eps=0.791\n",
            "Ep 0150/380 | Reward= -189.270 | eps=0.789\n",
            "Ep 0151/380 | Reward= -223.967 | eps=0.788\n",
            "Ep 0152/380 | Reward= -192.195 | eps=0.786\n",
            "Ep 0153/380 | Reward= -196.815 | eps=0.785\n",
            "Ep 0154/380 | Reward= -185.075 | eps=0.783\n",
            "Ep 0155/380 | Reward= -167.251 | eps=0.782\n",
            "Ep 0156/380 | Reward= -188.264 | eps=0.781\n",
            "Ep 0157/380 | Reward= -169.587 | eps=0.779\n",
            "Ep 0158/380 | Reward= -156.814 | eps=0.778\n",
            "Ep 0159/380 | Reward= -243.155 | eps=0.776\n",
            "Ep 0160/380 | Reward= -174.769 | eps=0.775\n",
            "  ValBIC | agent=-193109.507 | agent+CAM=-192724.060 | GraN-DAG=-203003.221\n",
            "  [ES] patience 6/10\n",
            "Ep 0161/380 | Reward= -204.914 | eps=0.774\n",
            "Ep 0162/380 | Reward= -163.482 | eps=0.772\n",
            "Ep 0163/380 | Reward= -200.396 | eps=0.771\n",
            "Ep 0164/380 | Reward= -187.728 | eps=0.769\n",
            "Ep 0165/380 | Reward= -201.390 | eps=0.768\n",
            "Ep 0166/380 | Reward= -167.099 | eps=0.767\n",
            "Ep 0167/380 | Reward= -136.394 | eps=0.765\n",
            "Ep 0168/380 | Reward= -147.213 | eps=0.764\n",
            "Ep 0169/380 | Reward= -172.368 | eps=0.762\n",
            "Ep 0170/380 | Reward= -241.492 | eps=0.761\n",
            "Ep 0171/380 | Reward= -288.739 | eps=0.760\n",
            "Ep 0172/380 | Reward= -200.955 | eps=0.758\n",
            "Ep 0173/380 | Reward= -252.841 | eps=0.757\n",
            "Ep 0174/380 | Reward= -185.345 | eps=0.755\n",
            "Ep 0175/380 | Reward= -208.040 | eps=0.754\n",
            "Ep 0176/380 | Reward= -163.232 | eps=0.753\n",
            "Ep 0177/380 | Reward= -144.121 | eps=0.751\n",
            "Ep 0178/380 | Reward= -263.879 | eps=0.750\n",
            "Ep 0179/380 | Reward= -193.868 | eps=0.748\n",
            "Ep 0180/380 | Reward= -164.529 | eps=0.747\n",
            "  ValBIC | agent=-192541.361 | agent+CAM=-192116.849 | GraN-DAG=-203003.221\n",
            "  [ES] patience 7/10\n",
            "Ep 0181/380 | Reward= -173.692 | eps=0.746\n",
            "Ep 0182/380 | Reward= -166.343 | eps=0.744\n",
            "Ep 0183/380 | Reward= -150.694 | eps=0.743\n",
            "Ep 0184/380 | Reward= -155.336 | eps=0.741\n",
            "Ep 0185/380 | Reward= -139.031 | eps=0.740\n",
            "Ep 0186/380 | Reward= -185.636 | eps=0.738\n",
            "Ep 0187/380 | Reward= -197.698 | eps=0.737\n",
            "Ep 0188/380 | Reward= -206.732 | eps=0.736\n",
            "Ep 0189/380 | Reward= -204.354 | eps=0.734\n",
            "Ep 0190/380 | Reward= -148.872 | eps=0.733\n",
            "Ep 0191/380 | Reward= -244.383 | eps=0.731\n",
            "Ep 0192/380 | Reward= -272.802 | eps=0.730\n",
            "Ep 0193/380 | Reward= -171.059 | eps=0.729\n",
            "Ep 0194/380 | Reward= -181.637 | eps=0.727\n",
            "Ep 0195/380 | Reward= -187.145 | eps=0.726\n",
            "Ep 0196/380 | Reward= -219.277 | eps=0.724\n",
            "Ep 0197/380 | Reward= -146.748 | eps=0.723\n",
            "Ep 0198/380 | Reward= -227.353 | eps=0.722\n",
            "Ep 0199/380 | Reward= -224.167 | eps=0.720\n",
            "Ep 0200/380 | Reward= -166.003 | eps=0.719\n",
            "  ValBIC | agent=-192522.181 | agent+CAM=-192009.845 | GraN-DAG=-203003.221\n",
            "  [SAVE] Best ValBIC improved to -192522.181428 → /content/agent_adj.npy\n",
            "Ep 0201/380 | Reward= -216.075 | eps=0.717\n",
            "Ep 0202/380 | Reward= -155.613 | eps=0.716\n",
            "Ep 0203/380 | Reward= -193.092 | eps=0.715\n",
            "Ep 0204/380 | Reward= -219.288 | eps=0.713\n",
            "Ep 0205/380 | Reward= -147.213 | eps=0.712\n",
            "Ep 0206/380 | Reward= -242.227 | eps=0.710\n",
            "Ep 0207/380 | Reward= -170.745 | eps=0.709\n",
            "Ep 0208/380 | Reward= -182.489 | eps=0.708\n",
            "Ep 0209/380 | Reward= -223.659 | eps=0.706\n",
            "Ep 0210/380 | Reward= -247.656 | eps=0.705\n",
            "Ep 0211/380 | Reward= -190.157 | eps=0.703\n",
            "Ep 0212/380 | Reward= -150.672 | eps=0.702\n",
            "Ep 0213/380 | Reward= -224.009 | eps=0.701\n",
            "Ep 0214/380 | Reward= -173.112 | eps=0.699\n",
            "Ep 0215/380 | Reward= -208.399 | eps=0.698\n",
            "Ep 0216/380 | Reward= -150.190 | eps=0.696\n",
            "Ep 0217/380 | Reward= -195.548 | eps=0.695\n",
            "Ep 0218/380 | Reward= -216.621 | eps=0.693\n",
            "Ep 0219/380 | Reward= -177.848 | eps=0.692\n",
            "Ep 0220/380 | Reward= -171.378 | eps=0.691\n",
            "  ValBIC | agent=-193005.828 | agent+CAM=-192508.352 | GraN-DAG=-203003.221\n",
            "  [ES] patience 1/10\n",
            "Ep 0221/380 | Reward= -189.774 | eps=0.689\n",
            "Ep 0222/380 | Reward= -222.682 | eps=0.688\n",
            "Ep 0223/380 | Reward= -230.382 | eps=0.686\n",
            "Ep 0224/380 | Reward= -221.493 | eps=0.685\n",
            "Ep 0225/380 | Reward= -149.357 | eps=0.684\n",
            "Ep 0226/380 | Reward= -216.767 | eps=0.682\n",
            "Ep 0227/380 | Reward= -224.040 | eps=0.681\n",
            "Ep 0228/380 | Reward= -154.853 | eps=0.679\n",
            "Ep 0229/380 | Reward= -223.580 | eps=0.678\n",
            "Ep 0230/380 | Reward= -193.825 | eps=0.677\n",
            "Ep 0231/380 | Reward= -183.000 | eps=0.675\n",
            "Ep 0232/380 | Reward= -222.738 | eps=0.674\n",
            "Ep 0233/380 | Reward= -142.676 | eps=0.672\n",
            "Ep 0234/380 | Reward= -181.785 | eps=0.671\n",
            "Ep 0235/380 | Reward= -259.393 | eps=0.670\n",
            "Ep 0236/380 | Reward= -180.558 | eps=0.668\n",
            "Ep 0237/380 | Reward= -151.488 | eps=0.667\n",
            "Ep 0238/380 | Reward= -179.911 | eps=0.665\n",
            "Ep 0239/380 | Reward= -150.866 | eps=0.664\n",
            "Ep 0240/380 | Reward= -168.541 | eps=0.663\n",
            "  ValBIC | agent=-192857.910 | agent+CAM=-192371.795 | GraN-DAG=-203003.221\n",
            "  [ES] patience 2/10\n",
            "Ep 0241/380 | Reward= -294.840 | eps=0.661\n",
            "Ep 0242/380 | Reward= -206.885 | eps=0.660\n",
            "Ep 0243/380 | Reward= -273.017 | eps=0.658\n",
            "Ep 0244/380 | Reward= -201.464 | eps=0.657\n",
            "Ep 0245/380 | Reward= -160.067 | eps=0.656\n",
            "Ep 0246/380 | Reward= -212.222 | eps=0.654\n",
            "Ep 0247/380 | Reward= -140.221 | eps=0.653\n",
            "Ep 0248/380 | Reward= -192.854 | eps=0.651\n",
            "Ep 0249/380 | Reward= -243.062 | eps=0.650\n",
            "Ep 0250/380 | Reward= -180.744 | eps=0.649\n",
            "Ep 0251/380 | Reward= -235.411 | eps=0.647\n",
            "Ep 0252/380 | Reward= -193.574 | eps=0.646\n",
            "Ep 0253/380 | Reward= -204.596 | eps=0.644\n",
            "Ep 0254/380 | Reward= -191.646 | eps=0.643\n",
            "Ep 0255/380 | Reward= -185.803 | eps=0.641\n",
            "Ep 0256/380 | Reward= -162.625 | eps=0.640\n",
            "Ep 0257/380 | Reward= -206.507 | eps=0.639\n",
            "Ep 0258/380 | Reward= -149.898 | eps=0.637\n",
            "Ep 0259/380 | Reward= -232.364 | eps=0.636\n",
            "Ep 0260/380 | Reward= -155.888 | eps=0.634\n",
            "  ValBIC | agent=-192310.883 | agent+CAM=-191807.698 | GraN-DAG=-203003.221\n",
            "  [SAVE] Best ValBIC improved to -192310.883128 → /content/agent_adj.npy\n",
            "Ep 0261/380 | Reward= -194.773 | eps=0.633\n",
            "Ep 0262/380 | Reward= -189.005 | eps=0.632\n",
            "Ep 0263/380 | Reward= -197.333 | eps=0.630\n",
            "Ep 0264/380 | Reward= -200.327 | eps=0.629\n",
            "Ep 0265/380 | Reward= -221.041 | eps=0.627\n",
            "Ep 0266/380 | Reward= -174.159 | eps=0.626\n",
            "Ep 0267/380 | Reward= -159.550 | eps=0.625\n",
            "Ep 0268/380 | Reward= -212.473 | eps=0.623\n",
            "Ep 0269/380 | Reward= -208.342 | eps=0.622\n",
            "Ep 0270/380 | Reward= -172.346 | eps=0.620\n",
            "Ep 0271/380 | Reward= -175.908 | eps=0.619\n",
            "Ep 0272/380 | Reward= -163.105 | eps=0.618\n",
            "Ep 0273/380 | Reward= -165.278 | eps=0.616\n",
            "Ep 0274/380 | Reward= -157.108 | eps=0.615\n",
            "Ep 0275/380 | Reward= -175.363 | eps=0.613\n",
            "Ep 0276/380 | Reward= -161.991 | eps=0.612\n",
            "Ep 0277/380 | Reward= -135.531 | eps=0.611\n",
            "Ep 0278/380 | Reward= -146.004 | eps=0.609\n",
            "Ep 0279/380 | Reward= -144.320 | eps=0.608\n",
            "Ep 0280/380 | Reward= -137.548 | eps=0.606\n",
            "  ValBIC | agent=-191967.420 | agent+CAM=-191588.712 | GraN-DAG=-203003.221\n",
            "  [SAVE] Best ValBIC improved to -191967.419720 → /content/agent_adj.npy\n",
            "Ep 0281/380 | Reward= -202.770 | eps=0.605\n",
            "Ep 0282/380 | Reward= -144.428 | eps=0.604\n",
            "Ep 0283/380 | Reward= -165.241 | eps=0.602\n",
            "Ep 0284/380 | Reward= -164.500 | eps=0.601\n",
            "Ep 0285/380 | Reward= -215.232 | eps=0.599\n",
            "Ep 0286/380 | Reward= -138.414 | eps=0.598\n",
            "Ep 0287/380 | Reward= -198.010 | eps=0.596\n",
            "Ep 0288/380 | Reward= -158.764 | eps=0.595\n",
            "Ep 0289/380 | Reward= -188.473 | eps=0.594\n",
            "Ep 0290/380 | Reward= -182.471 | eps=0.592\n",
            "Ep 0291/380 | Reward= -249.094 | eps=0.591\n",
            "Ep 0292/380 | Reward= -171.218 | eps=0.589\n",
            "Ep 0293/380 | Reward= -207.106 | eps=0.588\n",
            "Ep 0294/380 | Reward= -235.886 | eps=0.587\n",
            "Ep 0295/380 | Reward= -154.375 | eps=0.585\n",
            "Ep 0296/380 | Reward= -157.469 | eps=0.584\n",
            "Ep 0297/380 | Reward= -216.501 | eps=0.582\n",
            "Ep 0298/380 | Reward= -206.155 | eps=0.581\n",
            "Ep 0299/380 | Reward= -149.337 | eps=0.580\n",
            "Ep 0300/380 | Reward= -239.914 | eps=0.578\n",
            "  ValBIC | agent=-193336.290 | agent+CAM=-192827.590 | GraN-DAG=-203003.221\n",
            "  [ES] patience 1/10\n",
            "Ep 0301/380 | Reward= -142.372 | eps=0.577\n",
            "Ep 0302/380 | Reward= -184.705 | eps=0.575\n",
            "Ep 0303/380 | Reward= -175.177 | eps=0.574\n",
            "Ep 0304/380 | Reward= -157.956 | eps=0.573\n",
            "Ep 0305/380 | Reward= -169.914 | eps=0.571\n",
            "Ep 0306/380 | Reward= -188.351 | eps=0.570\n",
            "Ep 0307/380 | Reward= -223.683 | eps=0.568\n",
            "Ep 0308/380 | Reward= -158.062 | eps=0.567\n",
            "Ep 0309/380 | Reward= -185.442 | eps=0.566\n",
            "Ep 0310/380 | Reward= -169.366 | eps=0.564\n",
            "Ep 0311/380 | Reward= -152.770 | eps=0.563\n",
            "Ep 0312/380 | Reward= -208.395 | eps=0.561\n",
            "Ep 0313/380 | Reward= -234.790 | eps=0.560\n",
            "Ep 0314/380 | Reward= -163.394 | eps=0.559\n",
            "Ep 0315/380 | Reward= -179.477 | eps=0.557\n",
            "Ep 0316/380 | Reward= -203.820 | eps=0.556\n",
            "Ep 0317/380 | Reward= -201.578 | eps=0.554\n",
            "Ep 0318/380 | Reward= -174.947 | eps=0.553\n",
            "Ep 0319/380 | Reward= -236.810 | eps=0.551\n",
            "Ep 0320/380 | Reward= -199.716 | eps=0.550\n",
            "  ValBIC | agent=-193917.046 | agent+CAM=-193420.512 | GraN-DAG=-203003.221\n",
            "  [ES] patience 2/10\n",
            "Ep 0321/380 | Reward= -160.493 | eps=0.549\n",
            "Ep 0322/380 | Reward= -179.581 | eps=0.547\n",
            "Ep 0323/380 | Reward= -198.022 | eps=0.546\n",
            "Ep 0324/380 | Reward= -194.091 | eps=0.544\n",
            "Ep 0325/380 | Reward= -218.619 | eps=0.543\n",
            "Ep 0326/380 | Reward= -128.806 | eps=0.542\n",
            "Ep 0327/380 | Reward= -148.315 | eps=0.540\n",
            "Ep 0328/380 | Reward= -195.465 | eps=0.539\n",
            "Ep 0329/380 | Reward= -191.733 | eps=0.537\n",
            "Ep 0330/380 | Reward= -179.281 | eps=0.536\n",
            "Ep 0331/380 | Reward= -218.428 | eps=0.535\n",
            "Ep 0332/380 | Reward= -156.858 | eps=0.533\n",
            "Ep 0333/380 | Reward= -194.099 | eps=0.532\n",
            "Ep 0334/380 | Reward= -157.207 | eps=0.530\n",
            "Ep 0335/380 | Reward= -226.496 | eps=0.529\n",
            "Ep 0336/380 | Reward= -149.983 | eps=0.528\n",
            "Ep 0337/380 | Reward= -180.301 | eps=0.526\n",
            "Ep 0338/380 | Reward= -143.598 | eps=0.525\n",
            "Ep 0339/380 | Reward= -198.959 | eps=0.523\n",
            "Ep 0340/380 | Reward= -200.103 | eps=0.522\n",
            "  ValBIC | agent=-193930.906 | agent+CAM=-193554.504 | GraN-DAG=-203003.221\n",
            "  [ES] patience 3/10\n",
            "Ep 0341/380 | Reward= -195.658 | eps=0.521\n",
            "Ep 0342/380 | Reward= -190.688 | eps=0.519\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "GT_NPY = \"/content/adj.npy\"\n",
        "DATA_CSV   = \"/content/data.csv\"\n",
        "\n",
        "def load_data(csv_path=DATA_CSV):\n",
        "    if not os.path.exists(csv_path):\n",
        "        print(f\"Error: '{csv_path}' not found.\"); return None\n",
        "    df = pd.read_csv(csv_path, header=0)\n",
        "    if df.columns[0].lower().startswith(\"unnamed\"):\n",
        "        df = pd.read_csv(csv_path, header=0, index_col=0)\n",
        "    df = df.apply(pd.to_numeric, errors=\"coerce\").dropna(axis=1, how=\"all\")\n",
        "    X = df.values.astype(np.float64)\n",
        "    mu, sd = X.mean(0, keepdims=True), X.std(0, keepdims=True); sd[sd == 0] = 1.0\n",
        "    X = (X - mu) / sd\n",
        "    print(f\"Loaded data: {X.shape[0]} samples, {X.shape[1]} vars\")\n",
        "    return X\n",
        "\n",
        "\n",
        "\n",
        "def load_truth(npy_path=GT_NPY, p=None):\n",
        "    if not os.path.exists(npy_path):\n",
        "        print(\"No ground truth file; metrics will be limited.\")\n",
        "        return None\n",
        "    G = np.load(npy_path).astype(np.float64)\n",
        "    if p is not None and G.shape != (p, p):\n",
        "        print(f\"[align] trimming GT from {G.shape} to {(p,p)}\")\n",
        "        G = G[:p, :p]\n",
        "    print(\"Loaded ground truth:\", G.shape)\n",
        "    return G\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X = load_data(DATA_CSV)\n",
        "p = X.shape[1]\n",
        "GT = load_truth(GT_NPY, p=p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Usm6bfKDyIrx",
        "outputId": "06038369-8974-4c8f-ebc4-e53e5367b3eb"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded data: 10000 samples, 37 vars\n",
            "Loaded ground truth: (37, 37)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "A = np.load(\"/content/agent_adj.npy\")\n",
        "A.sum(), A.shape, A[:5,:5]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXcPRMgpk9HX",
        "outputId": "46f60078-32ea-4508-dd52-3f3af429bb94"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(np.int64(40),\n",
              " (37, 37),\n",
              " array([[0, 1, 0, 1, 0],\n",
              "        [0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 1, 0],\n",
              "        [0, 0, 0, 0, 0],\n",
              "        [0, 0, 1, 0, 0]]))"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_dYtgNBxqjn",
        "outputId": "0466a405-ba11-42d9-9c52-e061201b8906"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etWnAcw4xlXw",
        "outputId": "b456ebe8-1f32-4ae2-a44d-3aa06593ea78"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 1., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import logging\n",
        "import subprocess\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from typing import Callable, Optional\n",
        "\n",
        "\n",
        "def binarize_adj(A: np.ndarray) -> np.ndarray:\n",
        "    A = (A != 0).astype(int)\n",
        "    np.fill_diagonal(A, 0)\n",
        "    return A\n",
        "\n",
        "\n",
        "def shd_binary(A: np.ndarray, B: np.ndarray) -> int:\n",
        "    A = binarize_adj(A)\n",
        "    B = binarize_adj(B)\n",
        "    Au = ((A + A.T) > 0).astype(int)\n",
        "    Bu = ((B + B.T) > 0).astype(int)\n",
        "    undirected_diff = int(np.sum(np.triu(Au ^ Bu, 1)))\n",
        "    common_u = ((Au & Bu) > 0).astype(int)\n",
        "    orient_mismatch = int(np.sum(np.triu((A ^ B) & common_u, 1)))\n",
        "    return undirected_diff + orient_mismatch\n",
        "\n",
        "\n",
        "def compute_metrics(pred_adj: np.ndarray, gt_adj: np.ndarray) -> Optional[dict]:\n",
        "    if gt_adj is None or pred_adj.shape != gt_adj.shape:\n",
        "        return None\n",
        "    P = binarize_adj(pred_adj)\n",
        "    T = binarize_adj(gt_adj)\n",
        "    tp = int(((P == 1) & (T == 1)).sum())\n",
        "    fp = int(((P == 1) & (T == 0)).sum())\n",
        "    fn = int(((P == 0) & (T == 1)).sum())\n",
        "    tn = int(((P == 0) & (T == 0)).sum())\n",
        "    fdr = fp / max(tp + fp, 1)\n",
        "    tpr = tp / max(tp + fn, 1)\n",
        "    fpr = fp / max(fp + tn, 1)\n",
        "    return {\n",
        "        \"total_edges_gt\": int(T.sum()),\n",
        "        \"total_edges_pred\": int(P.sum()),\n",
        "        \"correct_edges\": tp,\n",
        "        \"fdr\": round(fdr, 4),\n",
        "        \"tpr\": round(tpr, 4),\n",
        "        \"fpr\": round(fpr, 4),\n",
        "        \"shd\": shd_binary(P, T),\n",
        "        \"nnz\": int(P.sum()),\n",
        "    }\n"
      ],
      "metadata": {
        "id": "vjsH34Sq2mAn"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A_final = binarize_adj(A)\n",
        "if GT is not None:\n",
        "    print(\"\\n--- Final (GT) Metrics ---\")\n",
        "    print(\"Agent (raw): \", compute_metrics(A_final, GT))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGUgkoC3mS2I",
        "outputId": "4202f861-2144-42eb-83d4-fc62c820bdeb"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Final (GT) Metrics ---\n",
            "Agent (raw):  {'total_edges_gt': 46, 'total_edges_pred': 40, 'correct_edges': 9, 'fdr': 0.775, 'tpr': 0.1957, 'fpr': 0.0234, 'shd': 64, 'nnz': 40}\n"
          ]
        }
      ]
    }
  ]
}